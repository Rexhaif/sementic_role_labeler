{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from typing import *\n",
    "\n",
    "import yargy as y\n",
    "import yargy.predicates as yp\n",
    "import yargy.morph as ytm\n",
    "import yargy.tokenizer as yt\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.processor_mystem import ProcessorMystem\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd \n",
    "\n",
    "from pyhash import city_32\n",
    "from rich import print, inspect\n",
    "import joblib as jb\n",
    "import json\n",
    "\n",
    "from rich import print\n",
    "import razdel\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from functools import lru_cache\n",
    "CACHE_SIZE=10000\n",
    "from src.contextual_morphology import UdMorphTokenizer, spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"results/results_youtube_emojipunct_new_morph.pkl\"\n",
    "RST_CACHE_NAME = \"cache/cache_w1.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy_udpipe.load_from_path(\n",
    "    lang=\"ru\",\n",
    "    path=\"./data/models/russian-syntagrus-ud-2.5-191206.udpipe\",\n",
    "    meta={\"description\": \"Custom 'hr' model\"}\n",
    ")\n",
    "ud_tokenizer = UdMorphTokenizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_morph(word, cyr=True):\n",
    "    morph = MorphAnalyzer()\n",
    "    if cyr:\n",
    "        return morph.lat2cyr(morph.parse(word)[0].tag)\n",
    "    else:\n",
    "        return morph.parse(word)[0].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = pd.read_csv(\"data/rules/rules_formatted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates_ = pd.read_csv(\"data/rules/predicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules['predicate_type'] = rules['require_deverbal_noun'].apply(lambda x: 'сущ' if x == '1' else 'глаг')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = rules[rules.require_deverbal_noun != '1']\n",
    "rules = rules[rules.require_reflexive != '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = set(predicates_.type.tolist())\n",
    "if 'deverbal_noun' in types:\n",
    "    deverbal_nouns = set(predicates_[predicates_.type == 'deverbal_noun']['predicate'].to_list())\n",
    "else:\n",
    "    deverbal_nouns = set()\n",
    "    \n",
    "if 'status_category' in types:\n",
    "    status_categories = set(predicates_[predicates_.type == 'status_category']['predicate'].to_list())\n",
    "else:\n",
    "    status_categories = set()\n",
    "    \n",
    "predicates = set(predicates_[predicates_.type == 'predicate']['predicate'].to_list())\n",
    "\n",
    "rule_specific = set(rules['predicate'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicates = predicates | rule_specific # | deverbal_nouns | status_categories |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicates -= {'?'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predicate_rule(\n",
    "    require_deverbal_noun: str,\n",
    "    require_reflexive: str,\n",
    "    require_status_category: str,\n",
    "    predicate: str,\n",
    "    predicate_type: str,\n",
    "    **kwargs\n",
    "):\n",
    "    rule_id = f\"predicate={predicate},deverbal={require_deverbal_noun},reflexive={require_reflexive},status_category={require_status_category},predicate_type={predicate_type}\"\n",
    "    return rule_id, y.rule(\n",
    "        y.and_(\n",
    "            yp.gram(\"VERB\")\n",
    "            #req_predicate(predicate, predicate_type),\n",
    "            #req_deverbal(require_deverbal_noun),\n",
    "            #req_reflexive(require_reflexive)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_argument_role(argument_type: str, case: str, preposition: str, **kwargs):\n",
    "    rule_id = f\"argument_type={argument_type},case={case},preposition={preposition}\"\n",
    "    arg = y.and_(\n",
    "        req_argument(),\n",
    "        req_animacy(argument_type),\n",
    "        req_case(case)\n",
    "    )\n",
    "    internal = y.or_(\n",
    "        y.and_(\n",
    "            yp.gram(\"ADJF\"), \n",
    "            y.or_(\n",
    "                yp.normalized(\"этот\"),\n",
    "                yp.normalized(\"тот\")\n",
    "            )\n",
    "        ),\n",
    "        y.not_(yp.gram(\"ADJF\"))\n",
    "    )\n",
    "    \n",
    "    rule = y.or_(\n",
    "        y.rule(req_preposition(preposition), arg),\n",
    "        y.rule(req_preposition(preposition), internal, arg)\n",
    "    )\n",
    "    return rule_id, rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_deverbal(require_deverbal_noun: str = '?'):\n",
    "    if require_deverbal_noun == '1': ## strictly deverbal noun\n",
    "        return y.and_(\n",
    "            yp.gram(\"NOUN\"),\n",
    "            yp.in_caseless(deverbal_nouns)\n",
    "        )\n",
    "    elif require_deverbal_noun == '0': ## strictly regular verb\n",
    "        return yp.gram(\"VERB\")\n",
    "        \n",
    "    elif require_deverbal_noun == '?': ## anything\n",
    "        return y.or_(\n",
    "            y.and_(\n",
    "                yp.gram(\"NOUN\"),\n",
    "                yp.in_caseless(deverbal_nouns)\n",
    "            ),\n",
    "            yp.gram(\"VERB\"),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect deverbal status\")\n",
    "    return yp.gram(\"VERB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_reflexive(reflexive_status: str = '?'):\n",
    "    \n",
    "    def is_reflexive_verb(verb: str):\n",
    "        return verb.endswith(\"ся\") or verb.endswith(\"сь\")\n",
    "    \n",
    "    if reflexive_status == \"1\":\n",
    "        return yp.custom(is_reflexive_verb)\n",
    "    if reflexive_status == \"0\":\n",
    "        return y.not_(yp.custom(is_reflexive_verb))\n",
    "    elif reflexive_status == \"?\":\n",
    "        return yp.true()\n",
    "    else:\n",
    "        raise ValueError (\"Incorrect reflexive status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_animacy(animacy: str = 'любой'):\n",
    "    if animacy == 'любой':\n",
    "        return yp.true()\n",
    "    elif animacy == 'одуш.':\n",
    "        return y.or_(\n",
    "            y.not_(yp.gram('Inan')),\n",
    "            yp.gram(\"Anim\")\n",
    "        )\n",
    "    elif animacy == 'неодуш.':\n",
    "        return y.or_(\n",
    "            yp.gram('Inan'),\n",
    "            y.not_(yp.gram(\"Anim\"))\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect Animacy Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_argument():\n",
    "    return y.and_(\n",
    "        y.not_(\n",
    "            y.or_( ## prohibits arguments from being any of following parts-of-speech\n",
    "                yp.gram('PART'),\n",
    "                yp.gram(\"ADP\"),\n",
    "                yp.gram(\"CCONJ\"),\n",
    "                yp.gram('SCONJ'),\n",
    "                yp.gram(\"INTJ\"),\n",
    "                yp.gram(\"ADJ\"),\n",
    "                yp.gram(\"VERB\")\n",
    "            )\n",
    "        ),\n",
    "        y.or_(\n",
    "            yp.gram(\"NOUN\"),\n",
    "            yp.gram(\"PROPN\"),\n",
    "            yp.gram(\"PRON\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_predicate(word: str = \"?\", predicate_type: str = 'глаг'):\n",
    "    if predicate_type == 'глаг':\n",
    "        predicate = yp.gram(\"VERB\")\n",
    "        \n",
    "    elif predicate_type == 'сущ':\n",
    "        predicate = yp.gram(\"NOUN\")\n",
    "        \n",
    "    elif predicate_type == 'любой':\n",
    "        predicate = y.or_(\n",
    "            yp.gram(\"VERB\"),\n",
    "            yp.gram(\"NOUN\")\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"predicate_type must be глаг or сущ or любой\")\n",
    "    if word != '?':\n",
    "        if \"|\" not in word:\n",
    "            # single-word scope\n",
    "            predicate = y.and_(\n",
    "                yp.normalized(word),\n",
    "                predicate\n",
    "            )\n",
    "        else:\n",
    "            predicate_words = word.split(\"|\")\n",
    "            scope_rule = list(map(yp.normalized, predicate_words))\n",
    "            scope_rule = y.or_(*scope_rule)\n",
    "            predicate = y.and_(\n",
    "                scope_rule,\n",
    "                predicate\n",
    "            )\n",
    "        \n",
    "    return predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_case(case: str = 'в'):\n",
    "    mapping = {\n",
    "        'в': 'Acc',\n",
    "        'т': 'Abl',\n",
    "        'д': 'Dat',\n",
    "        'р': 'Gen',\n",
    "        'и': 'Nom',\n",
    "        'п': 'Loc'\n",
    "    }\n",
    "    \n",
    "    if case not in mapping:\n",
    "        raise ValueError(f\"{case} unknown\")\n",
    "        \n",
    "    case_rule = yp.gram(mapping[case])\n",
    "    #del mapping[case]\n",
    "    #not_rule = y.not_(y.or_(*(yp.gram(other_case) for other_case in mapping.values())))\n",
    "    \n",
    "    return case_rule#, not_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_preposition(preposition: str = None):\n",
    "    preposition_rule = yp.eq(preposition)\n",
    "    \n",
    "    if preposition == 'None':\n",
    "        return y.not_(preposition_rule)\n",
    "    else:\n",
    "        return preposition_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_parser_pass(parser, text):\n",
    "    matches = []\n",
    "    for match in parser.findall(text):\n",
    "        matches.append({\n",
    "            'text': \" \".join([x.value for x in match.tokens]),\n",
    "            'span': tuple(match.span)\n",
    "        })\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_parser_pass(parser, text):\n",
    "    match = parser.match(text)\n",
    "    if match is not None:\n",
    "        matches = [{\n",
    "            'text': \" \".join([x.value for x in match.tokens]),\n",
    "            'span': tuple(match.span)\n",
    "        }]\n",
    "    else:\n",
    "        matches = []\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rules(**kwargs):\n",
    "    predicate_rule_id, predicate_rule = create_predicate_rule(**kwargs)\n",
    "    argument_rule_id, argument_rule = create_argument_role(**kwargs)\n",
    "    return {\n",
    "        'predicate_id': predicate_rule_id,\n",
    "        'argument_id': argument_rule_id,\n",
    "        'predicate_parser': y.Parser(predicate_rule, tokenizer=ud_tokenizer),\n",
    "        'argument_parser': y.Parser(argument_rule, tokenizer=ud_tokenizer)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleset = set(rules.role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2628ce02c6344a1fa9ceb44a2737c148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='экспериенцер'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31965ca750a44b5ab3ce289ac7cd15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='объект'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c546556ed14ec2b6da417e50a2a157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='инструмент'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9678654dfff4c95b2de0904d45bfa43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='каузатор'), FloatProgress(value=0.0, max=23.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for role in roleset:\n",
    "    ruleset[role] = []\n",
    "    \n",
    "    for rule_dict in tqdm(rules[rules.role == role].to_dict(orient='records'), desc=role):\n",
    "        ruleset[role].append(create_rules(**rule_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument_rules = {}\n",
    "for role in ruleset.keys():\n",
    "    for rule in ruleset[role]:\n",
    "        argument_rules[f\"{rule['argument_id']}+{role}\"] = {\n",
    "            'role':role,\n",
    "            'rule_id': rule['argument_id'],\n",
    "            'argument_parser': rule['argument_parser']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_orient = {}\n",
    "for role in ruleset.keys():\n",
    "    for rule in ruleset[role]:\n",
    "        rule_id = rule['predicate_id']\n",
    "        if rule_id not in predicate_orient:\n",
    "            predicate_orient[rule_id] = {}\n",
    "            predicate_orient[rule_id]['predicate_parser'] = rule['predicate_parser']\n",
    "            predicate_orient[rule_id]['arguments'] = []\n",
    "            \n",
    "        predicate_orient[rule_id]['arguments'].append({\n",
    "            'role': role,\n",
    "            'argument_id': rule['argument_id'],\n",
    "            'argument_parser': rule['argument_parser']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_orient_rules = {}\n",
    "for predicate_id in predicate_orient.keys():\n",
    "    argument_tuples = set([\n",
    "        f\"{x['argument_id']}+{x['role']}\" for x in predicate_orient[predicate_id]['arguments']\n",
    "    ])\n",
    "    predicate_orient_rules[predicate_id] = {\n",
    "        'predicate_parser': predicate_orient[predicate_id]['predicate_parser'],\n",
    "        'arguments': [argument_rules[key] for key in argument_tuples]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yargy.pipelines as pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pipeline = y.Parser(\n",
    "    pipelines.morph_pipeline(list(all_predicates)),\n",
    "    tokenizer=ud_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parseable(text, parser):\n",
    "    return len(list(parser.findall(text))) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgumentExtractor:\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def extract(self, sentence: str) -> List[Dict[str, Any]]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.predicate_argument_extract import UdPredicateArgumentExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline, ProcessingError\n",
    "from predpatt import PredPatt, load_conllu\n",
    "from predpatt.patt import Token\n",
    "from predpatt import PredPattOpts\n",
    "from predpatt.util.ud import dep_v1, dep_v2\n",
    "\n",
    "class PredPattArgumentExtractor(ArgumentExtractor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_to_udpipe: str,\n",
    "        resolve_relcl: bool = True,\n",
    "        resolve_appos: bool = True,\n",
    "        resolve_amod: bool = True,\n",
    "        resolve_conj: bool = True,\n",
    "        resolve_poss: bool = True,\n",
    "        ud = dep_v2.VERSION\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = Model.load(path_to_udpipe)\n",
    "        self.pipeline = Pipeline(self.model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "        self._error = ProcessingError()\n",
    "        self._opts = PredPattOpts(\n",
    "            resolve_relcl=resolve_relcl,\n",
    "            resolve_appos=resolve_appos,\n",
    "            resolve_amod=resolve_amod,\n",
    "            resolve_conj=resolve_conj,\n",
    "            resolve_poss=resolve_poss,\n",
    "            ud=ud\n",
    "        )\n",
    "        \n",
    "    @lru_cache(maxsize=100000)\n",
    "    def extract(self, sentence: str) -> List[Dict[str, Any]]:\n",
    "        processed = self.pipeline.process(sentence, self._error)\n",
    "        if self._error.occurred():\n",
    "            print(f\"=== Error occurred: {self._error.message}\")\n",
    "            self._error = ProcessingError()\n",
    "            return None\n",
    "        else:\n",
    "            conll_example = [ud_parse for sent_id, ud_parse in load_conllu(processed)][0]\n",
    "            ppatt = PredPatt(conll_example, opts=self._opts)\n",
    "            result = []\n",
    "            for predicate in ppatt.instances:\n",
    "                structure = {\n",
    "                    'predicate': predicate.tokens,\n",
    "                    'arguments': [x.tokens for x in predicate.arguments]\n",
    "                }\n",
    "                result.append(structure)\n",
    "                \n",
    "            return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class MainPhraseExtractor:\n",
    "    \n",
    "    def __init__(self, syntax_parser, pymorphy_analyzer):\n",
    "        self.syntax = syntax_parser\n",
    "        self.morph = pymorphy_analyzer\n",
    "\n",
    "    def get_main_phrase(self, words, get_prep=False, verbose=False):\n",
    "        markup = next(self.syntax.map([words]))\n",
    "        forward = {}\n",
    "        backward = defaultdict(list)\n",
    "        token_map = {}\n",
    "        candidates = []\n",
    "        for token in markup.tokens:\n",
    "            if token.head_id not in backward:\n",
    "                backward[token.head_id] = []\n",
    "\n",
    "            token_map[token.id] = token\n",
    "            forward[token.id] = token.head_id\n",
    "            backward[token.head_id].append(token.id)\n",
    "\n",
    "            if token.id == token.head_id or token.head_id == '0':\n",
    "                candidates.append(token.id)\n",
    "             \n",
    "        if verbose:\n",
    "            print(\"forward \", forward)\n",
    "            print(\"backward \", backward)\n",
    "            print(\"candidates \", candidates)\n",
    "                \n",
    "        if len(candidates) == 0:\n",
    "            return markup.tokens\n",
    "\n",
    "        candidate = sorted(candidates, key=lambda x: len(backward[x]))[-1]\n",
    "        if get_prep:\n",
    "            prep_candidates = backward[candidate]\n",
    "            prep_candidates = list(\n",
    "                filter(lambda x: self.morph.tag(token_map[x].text)[0].POS == 'PREP', prep_candidates)\n",
    "            )\n",
    "            if len(prep_candidates) == 0:\n",
    "                return [token_map[candidate]]\n",
    "            \n",
    "            prep = sorted(prep_candidates, key=lambda x: abs(int(x) - int(candidate)))[0]\n",
    "            return (token_map[prep], token_map[candidate])\n",
    "\n",
    "        return [token_map[candidate]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.processor_udpipe import ProcessorUDPipe\n",
    "class RstClauseSeparator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        udpipe_model: str = \"./data/models/russian-syntagrus-ud-2.5-191206.udpipe\", \n",
    "        rst=('papertext.ru', 5555),\n",
    "        cache_path=\"./rst-cache.pkl\"\n",
    "    ):\n",
    "        rst_host, rst_port = rst\n",
    "        self.cache_path = cache_path\n",
    "        self.ppl = PipelineCommon([\n",
    "            (ProcessorUDPipe(udpipe_model),\n",
    "             ['text'],\n",
    "             {'sentences': 'sentences',\n",
    "              'tokens': 'tokens',\n",
    "              'lemma': 'lemma',\n",
    "              'syntax_dep_tree': 'syntax_dep_tree',\n",
    "              'postag': 'ud_postag'}),\n",
    "            (ProcessorMystem(delay_init=False),\n",
    "             ['tokens', 'sentences'],\n",
    "             {'postag': 'postag'}),\n",
    "            (ConverterMystemToUd(),\n",
    "             ['postag'],\n",
    "             {'morph': 'morph',\n",
    "              'postag': 'postag'}),\n",
    "            (ProcessorRemote(rst_host, rst_port, 'default'),\n",
    "             ['text', 'tokens', 'sentences', 'postag', 'morph', 'lemma', 'syntax_dep_tree'],\n",
    "             {'clauses': 'clauses'})])\n",
    "        self.__cache = {}\n",
    "        self.__hasher = city_32()\n",
    "        if os.path.exists(self.cache_path):\n",
    "            self.__cache = jb.load(self.cache_path)\n",
    "        \n",
    "    def extract(self, text):\n",
    "        text_hash = self.__hasher(text)\n",
    "        if text_hash in self.__cache:\n",
    "            return self.__cache[text_hash]\n",
    "        else:\n",
    "            result = self.ppl(text)\n",
    "            clauses = [x.text for x in result['clauses']]\n",
    "            self.__cache[text_hash] = clauses\n",
    "            return clauses\n",
    "        \n",
    "        \n",
    "    def flush(self):\n",
    "        jb.dump(self.__cache, self.cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoleLabeler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        argument_extractor: ArgumentExtractor,\n",
    "        filter_pipeline,\n",
    "        predicate_ruleset,\n",
    "        mode: str = 'soft',\n",
    "        extend_arguments: bool = False,\n",
    "        drop_soft_predicates: bool = False\n",
    "    ):\n",
    "        \n",
    "        self.argument_extractor = argument_extractor\n",
    "        self.filter_pipeline = filter_pipeline\n",
    "        self.ruleset = predicate_ruleset\n",
    "        self.extend_arguments = extend_arguments\n",
    "        self.drop_soft_predicates = drop_soft_predicates\n",
    "        if mode == 'soft':\n",
    "            self.pass_fn = soft_parser_pass\n",
    "        elif mode == 'strict':\n",
    "            self.pass_fn = strict_parser_pass\n",
    "        else:\n",
    "            raise ValueError(f\"Incorrect mode = {mode}, can be 'soft' or 'strict'\")\n",
    "            \n",
    "    def check_parse(self, text, parser):\n",
    "        return len(self.pass_fn(parser, text)) > 0\n",
    "    \n",
    "    def run(self, sentence: str, return_applied_rules: bool = False):\n",
    "        tokenized = map(lambda x: x.text, razdel.tokenize(sentence))\n",
    "        words = list(map(lambda x: Token(x[0], x[1], None), enumerate(tokenized)))\n",
    "        arg_groups = self.argument_extractor.extract(sentence)\n",
    "        #arg_groups = list(\n",
    "        #    filter(\n",
    "        #        lambda x: check_parseable(\n",
    "        #            x['predicate'].text,\n",
    "        #            self.filter_pipeline\n",
    "        #        ),\n",
    "        #        arg_groups\n",
    "        #    )\n",
    "        #)\n",
    "        result = []\n",
    "        for group in arg_groups:\n",
    "            \n",
    "            predicate_txt = group['predicate'].text\n",
    "            predicate_tokens = [predicate_txt]\n",
    "            predicate_main = predicate_txt\n",
    "            forward_map = {argument.text: [argument] for argument in group['arguments']}\n",
    "            group_name = f\"predicate={predicate_txt},arguments=[{','.join(forward_map.keys())}]\"\n",
    "            group_result = []\n",
    "            \n",
    "            if predicate_main.endswith(\"ть\") or predicate_main.endswith(\"ться\"):\n",
    "                if self.drop_soft_predicates:\n",
    "                    continue\n",
    "                    \n",
    "            # iterating over predicates to find match\n",
    "            for predicate_id, predicate in self.ruleset.items():\n",
    "                if self.check_parse(predicate_main, predicate['predicate_parser']):\n",
    "                    predicate_result = {\n",
    "                        'predicate': predicate_txt,\n",
    "                        'predicate_analyzed': predicate_main,\n",
    "                        'predicate_morph': get_morph(predicate_main),\n",
    "                        'predicate_tokens': group['predicate'],\n",
    "                        'arguments': []\n",
    "                    }\n",
    "                    if return_applied_rules:\n",
    "                        predicate_result['applied_predicate_rule'] = predicate_id,\n",
    "\n",
    "                    for argument in forward_map.keys():\n",
    "                        argument_tokens = [x.text for x in forward_map[argument]]\n",
    "                        offset = min(x.position for x in forward_map[argument]) - 1\n",
    "                        argument_main_phrase = forward_map[argument]\n",
    "                        argument_main = \" \".join([\n",
    "                            x.text for x in argument_main_phrase\n",
    "                        ])\n",
    "                        argument_word = argument_main\n",
    "                        \n",
    "                        token_positions = [(offset + int(x.position)) for x in argument_main_phrase]\n",
    "                        try:\n",
    "                            if self.extend_arguments: # extending argument with up to 2 previous tokens to ensure preposition included\n",
    "                                min_pos = min(token_positions)\n",
    "                                if min_pos >= 2:\n",
    "                                    argument_main = f\"{words[min_pos - 2].text} {words[min_pos - 1].text} {argument_main}\"\n",
    "                                elif min_pos == 1:\n",
    "                                    argument_main = f\"{words[0].text} {argument_main}\"\n",
    "                        except IndexError as e:\n",
    "                            pass\n",
    "                            #print(f\"Index error for {token_positions} at {words}\")\n",
    "                        \n",
    "                        # iterating over possible arguments of matched predicate\n",
    "                        roles = []\n",
    "                        for argument_rule in predicate['arguments']:\n",
    "                            parser = argument_rule['argument_parser']\n",
    "                            rule_id = argument_rule['rule_id']\n",
    "                            role = argument_rule['role']\n",
    "                            if self.check_parse(argument_main, parser):\n",
    "                                #print(f\"Applied rule: {rule_id} to {argument_main}\")\n",
    "                                if return_applied_rules:\n",
    "                                    roles.append({\n",
    "                                        'role': role,\n",
    "                                        'applied_rule': rule_id\n",
    "                                    })\n",
    "                                else:\n",
    "                                    roles.append(role)\n",
    "                                \n",
    "                        if len(roles) > 0: \n",
    "                            predicate_result['arguments'].append({\n",
    "                                'argument': argument,\n",
    "                                'argument_main': argument_main,\n",
    "                                'argument_morph': get_morph(argument_main),\n",
    "                                'argument_analyzed': argument_word,\n",
    "                                'argument_tokens': forward_map[argument],\n",
    "                                'roles': tuple(roles)\n",
    "                            })\n",
    "                    if len(predicate_result['arguments']) > 0:\n",
    "                        predicate_result['arguments'] = tuple(predicate_result['arguments'])\n",
    "                        group_result.append(predicate_result)\n",
    "            result.append({'group': group_name, 'parses': group_result})\n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstraintEnforcer:\n",
    "    def __init__(self, constraints=None):\n",
    "        if constraints is None:\n",
    "            constraints = list()\n",
    "            \n",
    "        self.constraints = constraints\n",
    "        \n",
    "    def add(self, constraint):\n",
    "        self.constraints.append(constraint)\n",
    "        \n",
    "    def enforce(self, parse):\n",
    "        a_parse = parse.copy()\n",
    "        for constraint in self.constraints:\n",
    "            a_parse = constraint(a_parse)\n",
    "            if len(a_parse) == 0:\n",
    "                return a_parse\n",
    "            \n",
    "        return a_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = UdPredicateArgumentExtractor(\"./data/models/russian-syntagrus-ud-2.5-191206.udpipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_extractor = RstClauseSeparator(cache_path=\"./experiments/rst-cache.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = RoleLabeler(\n",
    "    extractor,\n",
    "    filter_pipeline, \n",
    "    predicate_orient_rules,\n",
    "    mode='soft',\n",
    "    extend_arguments=True,\n",
    "    drop_soft_predicates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "enforcer = ConstraintEnforcer()\n",
    "def enforce_parseable_predicate(parse):\n",
    "    if check_parseable(parse['predicate_analyzed'], filter_pipeline):\n",
    "        return parse\n",
    "    else:\n",
    "        return {}\n",
    "    \n",
    "def reduce_duplicate_roles(parse):\n",
    "    new_args = []\n",
    "    for arg in parse['arguments']:\n",
    "        arg['roles'] = tuple(set(arg['roles']))\n",
    "        new_args.append(arg)\n",
    "    parse['arguments'] = new_args\n",
    "    return parse\n",
    "    \n",
    "def resolve_multiple_expirirencers(parse):\n",
    "    if len(parse['arguments']) >= 2:\n",
    "        parse_roles = set(arg['roles'] for arg in parse['arguments'])\n",
    "        if ('экспериенцер',) in parse_roles:\n",
    "            new_args = []\n",
    "            for arg in parse['arguments']:\n",
    "                if len(arg['roles']) >= 2:\n",
    "                    new_roles = list(arg['roles'])\n",
    "                    if 'экспериенцер' in new_roles:\n",
    "                        new_roles.remove('экспериенцер')\n",
    "                    arg['roles'] = tuple(new_roles)\n",
    "                new_args.append(arg)\n",
    "            parse['arguments'] = new_args\n",
    "    return parse\n",
    "\n",
    "def resolve_single_expiriencer(parse):\n",
    "    parse_roles = [arg['roles'] for arg in parse['arguments'] if len(arg['roles']) >= 2]\n",
    "    if len(parse_roles) > 0:\n",
    "        n_exp = 0\n",
    "        for role in parse_roles:\n",
    "            if 'экспериенцер' in role:\n",
    "                n_exp += 1\n",
    "                \n",
    "        if n_exp == 1:\n",
    "            new_args = []\n",
    "            for arg in parse['arguments']:\n",
    "                if len(arg['roles']) >= 2 and 'экспериенцер' in arg['roles']:\n",
    "                    arg['roles'] = ('экспериенцер',)\n",
    "                new_args.append(arg)\n",
    "            parse['arguments'] = new_args\n",
    "    return parse\n",
    "        \n",
    "enforcer.add(enforce_parseable_predicate)\n",
    "enforcer.add(reduce_duplicate_roles) # ('каузатор', 'экспериенцер', 'экспериенцер') ->  ('каузатор', 'экспериенцер')\n",
    "enforcer.add(resolve_multiple_expirirencers)\n",
    "enforcer.add(resolve_single_expiriencer) # ('каузатор', 'экспериенцер') -> ('экспериенцер')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"./data/youtube/youtube_1yeat.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>У нас на всех ток шоу обсуждают всё кроме реал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>Не волнуйся! Все будет хорошо!!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>А есле полезити занимать кредит у МВФ мерового...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>@Александр Чемезов Я знаю и помню, когда не за...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>Глупость! Умный истеблишмент США стравливает п...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file                                               text\n",
       "0  D5S_deYcRqI  У нас на всех ток шоу обсуждают всё кроме реал...\n",
       "1  D5S_deYcRqI                  Не волнуйся! Все будет хорошо!!\\n\n",
       "2  D5S_deYcRqI  А есле полезити занимать кредит у МВФ мерового...\n",
       "3  D5S_deYcRqI  @Александр Чемезов Я знаю и помню, когда не за...\n",
       "4  D5S_deYcRqI  Глупость! Умный истеблишмент США стравливает п..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_punct(target: str, symbol: str = \";\"):\n",
    "    return re.sub(\n",
    "        r'{{(.*)}}',\n",
    "        symbol,\n",
    "        emoji.demojize(target, delimiters=(\"{{\", \"}}\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_apply(series, func, n_cores=4):\n",
    "    pool = Pool(n_cores)\n",
    "    series = pool.map(func, tqdm(series))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4441dcce264ec986e93a021dc465f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parallel_processed = parallel_apply(data['text'], emoji_to_punct, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = parallel_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63722832f299468c8108607e3c828201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clauses = []\n",
    "errors = []\n",
    "for text in tqdm(data['text'], total=len(data)):\n",
    "    text_clauses = []\n",
    "    for sentence in razdel.sentenize(text):\n",
    "        try:\n",
    "            text_clauses += clause_extractor.extract(sentence.text.replace(\". \", \"\"))\n",
    "        except Exception as e:\n",
    "            errors.append((sentence.text, e))\n",
    "            \n",
    "    text_clauses = list(map(str.lower, text_clauses))\n",
    "            \n",
    "    clauses.append(text_clauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clauses'] = clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet(\"./data/youtube/youtube_1year_preprocessed.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"./data/youtube/youtube_1year_preprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(596160, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(\"text\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(488087, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>clauses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>У нас на всех ток шоу обсуждают всё кроме реал...</td>\n",
       "      <td>[у нас на всех ток шоу обсуждают всё кроме реа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>Не волнуйся! Все будет хорошо!!\\n</td>\n",
       "      <td>[не волнуйся!, все будет хорошо!!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>А есле полезити занимать кредит у МВФ мерового...</td>\n",
       "      <td>[а есле полезити занимать кредит у мвф меровог...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>@Александр Чемезов Я знаю и помню, когда не за...</td>\n",
       "      <td>[@александр чемезов я знаю и помню, когда не з...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>Глупость! Умный истеблишмент США стравливает п...</td>\n",
       "      <td>[глупость!, умный истеблишмент сша стравливает...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file                                               text  \\\n",
       "0  D5S_deYcRqI  У нас на всех ток шоу обсуждают всё кроме реал...   \n",
       "1  D5S_deYcRqI                  Не волнуйся! Все будет хорошо!!\\n   \n",
       "2  D5S_deYcRqI  А есле полезити занимать кредит у МВФ мерового...   \n",
       "3  D5S_deYcRqI  @Александр Чемезов Я знаю и помню, когда не за...   \n",
       "4  D5S_deYcRqI  Глупость! Умный истеблишмент США стравливает п...   \n",
       "\n",
       "                                             clauses  \n",
       "0  [у нас на всех ток шоу обсуждают всё кроме реа...  \n",
       "1                 [не волнуйся!, все будет хорошо!!]  \n",
       "2  [а есле полезити занимать кредит у мвф меровог...  \n",
       "3  [@александр чемезов я знаю и помню, когда не з...  \n",
       "4  [глупость!, умный истеблишмент сша стравливает...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_clauses(clauses):\n",
    "    return list(filter(lambda x: check_parseable(x, filter_pipeline), clauses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b968ea7fe9974df4baba8a891d8a08ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_clauses = parallel_apply(data['clauses'][:100000], filter_clauses, n_cores=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_fn(clauses):\n",
    "    results = []\n",
    "    for clause in clauses:\n",
    "        try:\n",
    "            groups = labeler.run(clause)\n",
    "            for i, group in enumerate(groups):\n",
    "                for j, parse in enumerate(group['parses']):\n",
    "                    groups[i]['parses'][j] = enforcer.enforce(parse)\n",
    "            results.append(groups)\n",
    "        except Exception as e:\n",
    "            results.append([])\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbff4de1e104bdfbe045f469ec22d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parses = parallel_apply(filtered_clauses, parsing_fn, n_cores=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata = data[:10000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata['clauses'] = filtered_clauses\n",
    "subdata['parses'] = parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata.to_pickle(\"./result.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicate-Argument Structure Extraction works okay, problem is withing the rules and/or contextual morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-255281d45c8f>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subdata['clauses'] = filtered_clauses\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35be2a23538a4cb4b9789d1184d5f8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-255281d45c8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclause\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclauses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabeler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-331b27dbab7b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, sentence, return_applied_rules)\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;34m'predicate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredicate_txt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                         \u001b[0;34m'predicate_analyzed'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredicate_main\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                         \u001b[0;34m'predicate_morph'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_morph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicate_main\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                         \u001b[0;34m'predicate_tokens'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                         \u001b[0;34m'arguments'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-035b80d36b46>\u001b[0m in \u001b[0;36mget_morph\u001b[0;34m(word, cyr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_morph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcyr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmorph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMorphAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcyr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat2cyr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, lang, result_type, units, probability_estimator_cls, char_substitutes)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ru'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_dictionary_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36mchoose_dictionary_path\u001b[0;34m(cls, path, lang)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDICT_PATH_ENV_VARIABLE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlang_dict_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36mlang_dict_path\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlang_dict_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;34m\"\"\" Return language-specific dictionary path \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mlang_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lang_dict_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlang_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36m_lang_dict_paths\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m     paths = dict(\n\u001b[1;32m    121\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_iter_entry_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pymorphy2_dicts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36m_iter_entry_points\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWorkingSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_entry_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, entries)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36madd_entry\u001b[0;34m(self, entry)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfind_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mfind_on_path\u001b[0;34m(importer, path_item, only)\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0mfullpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2059\u001b[0m         \u001b[0mfactory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2060\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2061\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mdistributions_from_metadata\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m   2123\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2125\u001b[0;31m     yield Distribution.from_location(\n\u001b[0m\u001b[1;32m   2126\u001b[0m         \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecedence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVELOP_DIST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mfrom_location\u001b[0;34m(cls, location, basename, metadata, **kw)\u001b[0m\n\u001b[1;32m   2583\u001b[0m                     \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ver'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pyver'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'plat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m                 )\n\u001b[0;32m-> 2585\u001b[0;31m         return cls(\n\u001b[0m\u001b[1;32m   2586\u001b[0m             \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m             \u001b[0mpy_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpy_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_reload_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2983\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mitself\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2984\u001b[0m         \"\"\"\n\u001b[0;32m-> 2985\u001b[0;31m         \u001b[0mmd_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2986\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmd_version\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2987\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmd_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_get_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2766\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPKG_INFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2767\u001b[0;31m         \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_version_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2769\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_version_from_file\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'version:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m     \u001b[0mversion_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_version_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msafe_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_get_metadata\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2760\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2763\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36myield_lines\u001b[0;34m(strs)\u001b[0m\n\u001b[1;32m   2383\u001b[0m     \u001b[0;34m\"\"\"Yield non-empty/non-comment lines of a string or sequence\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2385\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2386\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m             \u001b[0;31m# skip blank lines/comments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parses = []\n",
    "empty_parses = []\n",
    "subdata = data[:10000]\n",
    "subdata['clauses'] = filtered_clauses\n",
    "for row in tqdm(subdata.itertuples(), total=len(subdata)):\n",
    "    text = row.text\n",
    "    file = row.file\n",
    "    clauses = row.clauses\n",
    "        \n",
    "    for i, clause in enumerate(clauses):\n",
    "        try:\n",
    "            groups = labeler.run(clause)\n",
    "            if len(groups) != 0:\n",
    "                for j, group in enumerate(groups):\n",
    "                    for k, parse in enumerate(group['parses']):\n",
    "                        parse = enforcer.enforce(parse)\n",
    "                        if len(parse) != 0:\n",
    "                            parses.append({\n",
    "                                'text': text,\n",
    "                                'clause_text': clause,\n",
    "                                'content_hash': file,\n",
    "                                'clause_idx': i,\n",
    "                                'group_idx': j,\n",
    "                                'parse_idx': k,\n",
    "                                'parse': parse\n",
    "                            })\n",
    "                        else:\n",
    "                            empty_parses.append({\n",
    "                                'text': text,\n",
    "                                'clause_text': clause,\n",
    "                                'content_hash': file,\n",
    "                                'clause_idx': i,\n",
    "                                'group_idx': j,\n",
    "                                'parse_idx': k,\n",
    "                                'parse': parse\n",
    "                                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error {e} with text {clause}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000\">'text'</span>: <span style=\"color: #008000\">'Да никто и не собирался успокаивать -ваша дурь, это чисто ваша проблема.\\n'</span>,\n",
       "    <span style=\"color: #008000\">'clause_text'</span>: <span style=\"color: #008000\">'да никто и не собирался успокаивать -ваша дурь, это чисто ваша </span>\n",
       "<span style=\"color: #008000\">проблема.'</span>,\n",
       "    <span style=\"color: #008000\">'content_hash'</span>: <span style=\"color: #008000\">'2nSp8ENwLGY'</span>,\n",
       "    <span style=\"color: #008000\">'clause_idx'</span>: <span style=\"color: #000080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #008000\">'group_idx'</span>: <span style=\"color: #000080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #008000\">'parse_idx'</span>: <span style=\"color: #000080; font-weight: bold\">1</span>,\n",
       "    <span style=\"color: #008000\">'parse'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000\">'predicate'</span>: <span style=\"color: #008000\">'успокаивать'</span>,\n",
       "        <span style=\"color: #008000\">'predicate_analyzed'</span>: <span style=\"color: #008000\">'успокаивать'</span>,\n",
       "        <span style=\"color: #008000\">'predicate_morph'</span>: <span style=\"color: #008000\">'ИНФ,несов,перех'</span>,\n",
       "        <span style=\"color: #008000\">'predicate_tokens'</span>: успокаивать/<span style=\"color: #000080; font-weight: bold\">5</span>,\n",
       "        <span style=\"color: #008000\">'arguments'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000\">'argument'</span>: <span style=\"color: #008000\">'никто'</span>,\n",
       "                <span style=\"color: #008000\">'argument_main'</span>: <span style=\"color: #008000\">'да никто'</span>,\n",
       "                <span style=\"color: #008000\">'argument_morph'</span>: <span style=\"color: #008000\">'Н'</span>,\n",
       "                <span style=\"color: #008000\">'argument_analyzed'</span>: <span style=\"color: #008000\">'никто'</span>,\n",
       "                <span style=\"color: #008000\">'argument_tokens'</span>: <span style=\"font-weight: bold\">[</span>никто/<span style=\"color: #000080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #008000\">'roles'</span>: <span style=\"font-weight: bold\">(</span><span style=\"color: #008000\">'экспериенцер'</span>,<span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f1378a78880>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(parses[-30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_extractor.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results/results_youtube_new_morph.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump({'full_parses': parses, 'empty_parses': empty_parses}, \"results/results_youtube_new_morph.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Done\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f727c1b2670>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
