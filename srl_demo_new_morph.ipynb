{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from typing import *\n",
    "\n",
    "import yargy as y\n",
    "import yargy.predicates as yp\n",
    "import yargy.morph as ytm\n",
    "import yargy.tokenizer as yt\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.processor_mystem import ProcessorMystem\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd \n",
    "\n",
    "from pyhash import city_32\n",
    "from rich import print, inspect\n",
    "import joblib as jb\n",
    "import json\n",
    "\n",
    "from rich import print\n",
    "import razdel\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from functools import lru_cache\n",
    "CACHE_SIZE=10000\n",
    "from src.contextual_morphology import UdMorphTokenizer, spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"results/results_youtube_emojipunct.pkl\"\n",
    "RST_CACHE_NAME = \"cache/cache_w1.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy_udpipe.load_from_path(\n",
    "    lang=\"ru\",\n",
    "    path=\"./data/models/russian-syntagrus-ud-2.5-191206.udpipe\",\n",
    "    meta={\"description\": \"Custom 'hr' model\"}\n",
    ")\n",
    "ud_tokenizer = UdMorphTokenizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_morph(word, cyr=True):\n",
    "    morph = MorphAnalyzer()\n",
    "    if cyr:\n",
    "        return morph.lat2cyr(morph.parse(word)[0].tag)\n",
    "    else:\n",
    "        return morph.parse(word)[0].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = pd.read_csv(\"data/rules/rules_formatted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates_ = pd.read_csv(\"data/rules/predicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = set(predicates_.type.tolist())\n",
    "if 'deverbal_noun' in types:\n",
    "    deverbal_nouns = set(predicates_[predicates_.type == 'deverbal_noun']['predicate'].to_list())\n",
    "else:\n",
    "    deverbal_nouns = set()\n",
    "    \n",
    "if 'status_category' in types:\n",
    "    status_categories = set(predicates_[predicates_.type == 'status_category']['predicate'].to_list())\n",
    "else:\n",
    "    status_categories = set()\n",
    "    \n",
    "predicates = set(predicates_[predicates_.type == 'predicate']['predicate'].to_list())\n",
    "\n",
    "rule_specific = set(rules['predicate'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicates = predicates | rule_specific # | deverbal_nouns | status_categories |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicates -= {'?'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predicate_rule(\n",
    "    require_deverbal_noun: str,\n",
    "    require_reflexive: str,\n",
    "    require_status_category: str,\n",
    "    predicate: str,\n",
    "    predicate_type: str,\n",
    "    **kwargs\n",
    "):\n",
    "    rule_id = f\"predicate={predicate},deverbal={require_deverbal_noun},reflexive={require_reflexive},status_category={require_status_category},predicate_type={predicate_type}\"\n",
    "    return rule_id, y.rule(\n",
    "        y.and_(\n",
    "            req_predicate(predicate, predicate_type),\n",
    "            req_deverbal(require_deverbal_noun),\n",
    "            req_reflexive(require_reflexive)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_argument_role(argument_type: str, case: str, preposition: str, **kwargs):\n",
    "    rule_id = f\"argument_type={argument_type},case={case},preposition={preposition}\"\n",
    "    arg = y.and_(\n",
    "        req_argument(),\n",
    "        req_animacy(argument_type),\n",
    "        req_case(case)\n",
    "    )\n",
    "    internal = y.or_(\n",
    "        y.and_(\n",
    "            yp.gram(\"ADJF\"), \n",
    "            y.or_(\n",
    "                yp.normalized(\"этот\"),\n",
    "                yp.normalized(\"тот\")\n",
    "            )\n",
    "        ),\n",
    "        y.not_(yp.gram(\"ADJF\"))\n",
    "    )\n",
    "    \n",
    "    rule = y.or_(\n",
    "        y.rule(req_preposition(preposition), arg),\n",
    "        y.rule(req_preposition(preposition), internal, arg)\n",
    "    )\n",
    "    return rule_id, rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_deverbal(require_deverbal_noun: str = '?'):\n",
    "    if require_deverbal_noun == '1': ## strictly deverbal noun\n",
    "        return y.and_(\n",
    "            yp.gram(\"NOUN\"),\n",
    "            yp.in_caseless(deverbal_nouns)\n",
    "        )\n",
    "    elif require_deverbal_noun == '0': ## strictly regular verb\n",
    "        return y.or_(\n",
    "            yp.gram(\"VERB\"),\n",
    "            #yp.gram(\"INFN\") UD does not have infn\n",
    "        )\n",
    "    elif require_deverbal_noun == '?': ## anything\n",
    "        return y.or_(\n",
    "            y.and_(\n",
    "                yp.gram(\"NOUN\"),\n",
    "                yp.in_caseless(deverbal_nouns)\n",
    "            ),\n",
    "            yp.gram(\"VERB\"),\n",
    "            #yp.gram(\"INFN\") ud does not have infn\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect deverbal status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_reflexive(reflexive_status: str = '?'):\n",
    "    \n",
    "    def is_reflexive_verb(verb: str):\n",
    "        return verb.endswith(\"ся\") or verb.endswith(\"сь\")\n",
    "    \n",
    "    if reflexive_status == \"1\":\n",
    "        return yp.custom(is_reflexive_verb)\n",
    "    if reflexive_status == \"0\":\n",
    "        return y.not_(yp.custom(is_reflexive_verb))\n",
    "    elif reflexive_status == \"?\":\n",
    "        return yp.true()\n",
    "    else:\n",
    "        raise ValueError (\"Incorrect reflexive status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_animacy(animacy: str = 'любой'):\n",
    "    if animacy == 'любой':\n",
    "        return yp.true()\n",
    "    elif animacy == 'одуш.':\n",
    "        return y.or_(\n",
    "            y.not_(yp.gram('Inan')),\n",
    "            yp.gram(\"Anim\"),\n",
    "            yp.gram(\"NOUN\"),\n",
    "            yp.gram(\"ADJ\")\n",
    "        )\n",
    "    elif animacy == 'неодуш.':\n",
    "        return y.or_(\n",
    "            yp.gram('Inan'),\n",
    "            y.not_(yp.gram(\"Anim\")),\n",
    "            #yp.gram(\"anim\"),\n",
    "            #yp.gram(\"NPRO\"), considering pronouns as animacy now, todo - improve it\n",
    "            yp.gram(\"ADJ\")\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect Animacy Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_argument():\n",
    "    return y.and_(\n",
    "        y.not_(\n",
    "            y.or_( ## prohibits arguments from being any of following parts-of-speech\n",
    "                yp.gram('PART'),\n",
    "                yp.gram(\"ADP\"),\n",
    "                yp.gram(\"CCONJ\"),\n",
    "                yp.gram('SCONJ'),\n",
    "                yp.gram(\"INTJ\"),\n",
    "                yp.gram(\"ADJ\"),\n",
    "                yp.gram(\"VERB\")\n",
    "            )\n",
    "        ),\n",
    "        y.or_(\n",
    "            yp.gram(\"NOUN\"),\n",
    "            yp.gram(\"PROPN\"),\n",
    "            yp.gram(\"PRON\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_predicate(word: str = \"?\", predicate_type: str = 'глаг'):\n",
    "    # add predicate_type handling\n",
    "    if predicate_type == 'глаг':\n",
    "        predicate = y.or_(\n",
    "            yp.gram(\"VERB\"),\n",
    "            #yp.gram(\"INFN\")\n",
    "        )\n",
    "    elif predicate_type == 'сущ':\n",
    "        predicate = y.or_(\n",
    "            #yp.gram(\"INFN\"),\n",
    "            yp.gram(\"NOUN\")\n",
    "        )\n",
    "    elif predicate_type == 'любой':\n",
    "        predicate = y.or_(\n",
    "            yp.gram(\"VERB\"),\n",
    "            #yp.gram(\"INFN\"),\n",
    "            yp.gram(\"NOUN\")\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"predicate_type must be глаг or сущ or любой\")\n",
    "    if word != '?':\n",
    "        if \"|\" not in word:\n",
    "            # single-word scope\n",
    "            predicate = y.and_(\n",
    "                yp.normalized(word),\n",
    "                predicate\n",
    "            )\n",
    "        else:\n",
    "            predicate_words = word.split(\"|\")\n",
    "            scope_rule = list(map(yp.normalized, predicate_words))\n",
    "            scope_rule = y.or_(*scope_rule)\n",
    "            predicate = y.and_(\n",
    "                scope_rule,\n",
    "                predicate\n",
    "            )\n",
    "        \n",
    "    return predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_case(case: str = 'в'):\n",
    "    mapping = {\n",
    "        'в': 'Acc',\n",
    "        'т': 'Abl',\n",
    "        'д': 'Dat',\n",
    "        'р': 'Gen',\n",
    "        'и': 'Nom',\n",
    "        'п': 'Loc'\n",
    "    }\n",
    "    \n",
    "    if case not in mapping:\n",
    "        raise ValueError(f\"{case} unknown\")\n",
    "        \n",
    "    case_rule = yp.gram(mapping[case])\n",
    "    del mapping[case]\n",
    "    not_rule = y.not_(y.or_(*(yp.gram(other_case) for other_case in mapping.values())))\n",
    "    \n",
    "    return y.and_(case_rule, not_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_preposition(preposition: str = None):\n",
    "    if preposition == 'None':\n",
    "        return yp.true()\n",
    "    else:\n",
    "        return y.or_(\n",
    "            y.and_(\n",
    "                yp.gram(\"ADP\"),\n",
    "                yp.eq(preposition)\n",
    "            )#,\n",
    "            #y.not_(yp.gram(\"PREP\"))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_parser_pass(parser, text):\n",
    "    matches = []\n",
    "    for match in parser.findall(text):\n",
    "        matches.append({\n",
    "            'text': \" \".join([x.value for x in match.tokens]),\n",
    "            'span': tuple(match.span)\n",
    "        })\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_parser_pass(parser, text):\n",
    "    match = parser.match(text)\n",
    "    if match is not None:\n",
    "        matches = [{\n",
    "            'text': \" \".join([x.value for x in match.tokens]),\n",
    "            'span': tuple(match.span)\n",
    "        }]\n",
    "    else:\n",
    "        matches = []\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rules(**kwargs):\n",
    "    predicate_rule_id, predicate_rule = create_predicate_rule(**kwargs)\n",
    "    argument_rule_id, argument_rule = create_argument_role(**kwargs)\n",
    "    return {\n",
    "        'predicate_id': predicate_rule_id,\n",
    "        'argument_id': argument_rule_id,\n",
    "        'predicate_parser': y.Parser(predicate_rule, tokenizer=ud_tokenizer),\n",
    "        'argument_parser': y.Parser(argument_rule, tokenizer=ud_tokenizer)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleset = set(rules.role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc6a1f694504b30943c1165d920f9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='каузатив'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87f04cd08724fa28a8ba2e5b93dc3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='объект'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd2b55f33fc4b10a848754aaf1b9e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='каузатор'), FloatProgress(value=0.0, max=31.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5ea325e0d4412cbc05c1b425698c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='инструмент'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f3ff619868458da860365ae8dbfdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='экспериенцер'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for role in roleset:\n",
    "    ruleset[role] = []\n",
    "    \n",
    "    for rule_dict in tqdm(rules[rules.role == role].to_dict(orient='records'), desc=role):\n",
    "        ruleset[role].append(create_rules(**rule_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument_rules = {}\n",
    "for role in ruleset.keys():\n",
    "    for rule in ruleset[role]:\n",
    "        argument_rules[f\"{rule['argument_id']}+{role}\"] = {\n",
    "            'role':role,\n",
    "            'rule_id': rule['argument_id'],\n",
    "            'argument_parser': rule['argument_parser']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_orient = {}\n",
    "for role in ruleset.keys():\n",
    "    for rule in ruleset[role]:\n",
    "        rule_id = rule['predicate_id']\n",
    "        if rule_id not in predicate_orient:\n",
    "            predicate_orient[rule_id] = {}\n",
    "            predicate_orient[rule_id]['predicate_parser'] = rule['predicate_parser']\n",
    "            predicate_orient[rule_id]['arguments'] = []\n",
    "            \n",
    "        predicate_orient[rule_id]['arguments'].append({\n",
    "            'role': role,\n",
    "            'argument_id': rule['argument_id'],\n",
    "            'argument_parser': rule['argument_parser']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_orient_rules = {}\n",
    "for predicate_id in predicate_orient.keys():\n",
    "    argument_tuples = set([\n",
    "        f\"{x['argument_id']}+{x['role']}\" for x in predicate_orient[predicate_id]['arguments']\n",
    "    ])\n",
    "    predicate_orient_rules[predicate_id] = {\n",
    "        'predicate_parser': predicate_orient[predicate_id]['predicate_parser'],\n",
    "        'arguments': [argument_rules[key] for key in argument_tuples]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yargy.pipelines as pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pipeline = y.Parser(\n",
    "    pipelines.morph_pipeline(list(all_predicates)),\n",
    "    tokenizer=ud_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parseable(text, parser):\n",
    "    return len(list(parser.findall(text))) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgumentExtractor:\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def extract(self, sentence: str) -> List[Dict[str, Any]]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline, ProcessingError\n",
    "from predpatt import PredPatt, load_conllu\n",
    "from predpatt.patt import Token\n",
    "from predpatt import PredPattOpts\n",
    "from predpatt.util.ud import dep_v1, dep_v2\n",
    "\n",
    "class PredPattArgumentExtractor(ArgumentExtractor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_to_udpipe: str,\n",
    "        resolve_relcl: bool = True,\n",
    "        resolve_appos: bool = True,\n",
    "        resolve_amod: bool = True,\n",
    "        resolve_conj: bool = True,\n",
    "        resolve_poss: bool = True,\n",
    "        ud = dep_v2.VERSION\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = Model.load(path_to_udpipe)\n",
    "        self.pipeline = Pipeline(self.model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "        self._error = ProcessingError()\n",
    "        self._opts = PredPattOpts(\n",
    "            resolve_relcl=resolve_relcl,\n",
    "            resolve_appos=resolve_appos,\n",
    "            resolve_amod=resolve_amod,\n",
    "            resolve_conj=resolve_conj,\n",
    "            resolve_poss=resolve_poss,\n",
    "            ud=ud\n",
    "        )\n",
    "        \n",
    "    @lru_cache(maxsize=100000)\n",
    "    def extract(self, sentence: str) -> List[Dict[str, Any]]:\n",
    "        processed = self.pipeline.process(sentence, self._error)\n",
    "        if self._error.occurred():\n",
    "            print(f\"=== Error occurred: {self._error.message}\")\n",
    "            self._error = ProcessingError()\n",
    "            return None\n",
    "        else:\n",
    "            conll_example = [ud_parse for sent_id, ud_parse in load_conllu(processed)][0]\n",
    "            ppatt = PredPatt(conll_example, opts=self._opts)\n",
    "            result = []\n",
    "            for predicate in ppatt.instances:\n",
    "                structure = {\n",
    "                    'predicate': predicate.tokens,\n",
    "                    'arguments': [x.tokens for x in predicate.arguments]\n",
    "                }\n",
    "                result.append(structure)\n",
    "                \n",
    "            return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class MainPhraseExtractor:\n",
    "    \n",
    "    def __init__(self, syntax_parser, pymorphy_analyzer):\n",
    "        self.syntax = syntax_parser\n",
    "        self.morph = pymorphy_analyzer\n",
    "\n",
    "    def get_main_phrase(self, words, get_prep=False, verbose=False):\n",
    "        markup = next(self.syntax.map([words]))\n",
    "        forward = {}\n",
    "        backward = defaultdict(list)\n",
    "        token_map = {}\n",
    "        candidates = []\n",
    "        for token in markup.tokens:\n",
    "            if token.head_id not in backward:\n",
    "                backward[token.head_id] = []\n",
    "\n",
    "            token_map[token.id] = token\n",
    "            forward[token.id] = token.head_id\n",
    "            backward[token.head_id].append(token.id)\n",
    "\n",
    "            if token.id == token.head_id or token.head_id == '0':\n",
    "                candidates.append(token.id)\n",
    "             \n",
    "        if verbose:\n",
    "            print(\"forward \", forward)\n",
    "            print(\"backward \", backward)\n",
    "            print(\"candidates \", candidates)\n",
    "                \n",
    "        if len(candidates) == 0:\n",
    "            return markup.tokens\n",
    "\n",
    "        candidate = sorted(candidates, key=lambda x: len(backward[x]))[-1]\n",
    "        if get_prep:\n",
    "            prep_candidates = backward[candidate]\n",
    "            prep_candidates = list(\n",
    "                filter(lambda x: self.morph.tag(token_map[x].text)[0].POS == 'PREP', prep_candidates)\n",
    "            )\n",
    "            if len(prep_candidates) == 0:\n",
    "                return [token_map[candidate]]\n",
    "            \n",
    "            prep = sorted(prep_candidates, key=lambda x: abs(int(x) - int(candidate)))[0]\n",
    "            return (token_map[prep], token_map[candidate])\n",
    "\n",
    "        return [token_map[candidate]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RstClauseSeparator:\n",
    "    def __init__(self, udpipe=('tsa05.isa.ru', 3334), rst=('papertext.ru', 5555), cache_path=\"./rst-cache.pkl\"):\n",
    "        udpipe_host, udpipe_port = udpipe\n",
    "        rst_host, rst_port = rst\n",
    "        self.cache_path = cache_path\n",
    "        self.ppl = PipelineCommon([\n",
    "            (ProcessorRemote(udpipe_host, udpipe_port, '0'),\n",
    "             ['text'],\n",
    "             {'sentences': 'sentences',\n",
    "              'tokens': 'tokens',\n",
    "              'lemma': 'lemma',\n",
    "              'syntax_dep_tree': 'syntax_dep_tree',\n",
    "              'postag': 'ud_postag'}),\n",
    "            (ProcessorMystem(delay_init=False),\n",
    "             ['tokens', 'sentences'],\n",
    "             {'postag': 'postag'}),\n",
    "            (ConverterMystemToUd(),\n",
    "             ['postag'],\n",
    "             {'morph': 'morph',\n",
    "              'postag': 'postag'}),\n",
    "            (ProcessorRemote(rst_host, rst_port, 'default'),\n",
    "             ['text', 'tokens', 'sentences', 'postag', 'morph', 'lemma', 'syntax_dep_tree'],\n",
    "             {'clauses': 'clauses'})])\n",
    "        self.__cache = {}\n",
    "        self.__hasher = city_32()\n",
    "        if os.path.exists(self.cache_path):\n",
    "            self.__cache = jb.load(self.cache_path)\n",
    "        \n",
    "    def extract(self, text):\n",
    "        text_hash = self.__hasher(text)\n",
    "        if text_hash in self.__cache:\n",
    "            return self.__cache[text_hash]\n",
    "        else:\n",
    "            result = self.ppl(text)\n",
    "            clauses = [x.text for x in result['clauses']]\n",
    "            self.__cache[text_hash] = clauses\n",
    "            return clauses\n",
    "        \n",
    "        \n",
    "    def flush(self):\n",
    "        jb.dump(self.__cache, self.cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoleLabeler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        argument_extractor: ArgumentExtractor,\n",
    "        main_phrase_extractor: MainPhraseExtractor,\n",
    "        filter_pipeline,\n",
    "        predicate_ruleset,\n",
    "        mode: str = 'soft',\n",
    "        extend_arguments: bool = False\n",
    "    ):\n",
    "        \n",
    "        self.argument_extractor = argument_extractor\n",
    "        self.main_phrase_extractor = main_phrase_extractor\n",
    "        self.filter_pipeline = filter_pipeline\n",
    "        self.ruleset = predicate_ruleset\n",
    "        self.extend_arguments = extend_arguments\n",
    "        if mode == 'soft':\n",
    "            self.pass_fn = soft_parser_pass\n",
    "        elif mode == 'strict':\n",
    "            self.pass_fn = strict_parser_pass\n",
    "        else:\n",
    "            raise ValueError(f\"Incorrect mode = {mode}, can be 'soft' or 'strict'\")\n",
    "            \n",
    "    def check_parse(self, text, parser):\n",
    "        return len(self.pass_fn(parser, text)) > 0\n",
    "    \n",
    "    def run(self, sentence: str, return_applied_rules: bool = False):\n",
    "        tokenized = map(lambda x: x.text, razdel.tokenize(sentence))\n",
    "        words = list(map(lambda x: Token(x[0], x[1], None), enumerate(tokenized)))\n",
    "        arg_groups = self.argument_extractor.extract(sentence)\n",
    "        arg_groups = list(\n",
    "            filter(\n",
    "                lambda x: check_parseable(\n",
    "                    \" \".join([token.text for token in x['predicate']]),\n",
    "                    self.filter_pipeline\n",
    "                ),\n",
    "                arg_groups\n",
    "            )\n",
    "        )\n",
    "        result = []\n",
    "        for group in arg_groups:\n",
    "            \n",
    "            predicate_txt = \" \".join([token.text for token in group['predicate']])\n",
    "            predicate_tokens = [token.text for token in group['predicate']]\n",
    "            predicate_main = \" \".join([x.text for x in self.main_phrase_extractor.get_main_phrase(predicate_tokens)])\n",
    "            forward_map = {\" \".join([token.text for token in argument]): argument for argument in group['arguments']}\n",
    "            group_name = f\"predicate={predicate_txt},arguments=[{','.join(forward_map.keys())}]\"\n",
    "            group_result = []\n",
    "            \n",
    "            # iterating over predicates to find match\n",
    "            for predicate_id, predicate in self.ruleset.items():\n",
    "                if self.check_parse(predicate_main, predicate['predicate_parser']):\n",
    "                    predicate_result = {\n",
    "                        'predicate': predicate_txt,\n",
    "                        'predicate_analyzed': predicate_main,\n",
    "                        'predicate_morph': get_morph(predicate_main),\n",
    "                        'predicate_tokens': group['predicate'],\n",
    "                        'arguments': []\n",
    "                    }\n",
    "                    if return_applied_rules:\n",
    "                        predicate_result['applied_predicate_rule'] = predicate_id,\n",
    "\n",
    "                    for argument in forward_map.keys():\n",
    "                        argument_tokens = [x.text for x in forward_map[argument]]\n",
    "                        offset = min(x.position for x in forward_map[argument]) - 1\n",
    "                        argument_main_phrase = self.main_phrase_extractor.get_main_phrase(argument_tokens, True)\n",
    "                        argument_main = \" \".join([\n",
    "                            x.text for x in argument_main_phrase\n",
    "                        ])\n",
    "                        argument_word = argument_main\n",
    "                        \n",
    "                        token_positions = [(offset + int(x.id)) for x in argument_main_phrase]\n",
    "                        try:\n",
    "                            if self.extend_arguments: # extending argument with up to 2 previous tokens to ensure preposition included\n",
    "                                min_pos = min(token_positions)\n",
    "                                if min_pos >= 2:\n",
    "                                    argument_main = f\"{words[min_pos - 2].text} {words[min_pos - 1].text} {argument_main}\"\n",
    "                                elif min_pos == 1:\n",
    "                                    argument_main = f\"{words[0].text} {argument_main}\"\n",
    "                        except IndexError as e:\n",
    "                            print(f\"Index error for {token_positions} at {words}\")\n",
    "                        \n",
    "                        # iterating over possible arguments of matched predicate\n",
    "                        roles = []\n",
    "                        for argument_rule in predicate['arguments']:\n",
    "                            parser = argument_rule['argument_parser']\n",
    "                            rule_id = argument_rule['rule_id']\n",
    "                            role = argument_rule['role']\n",
    "                            if self.check_parse(argument_main, parser):\n",
    "                                #print(f\"Applied rule: {rule_id} to {argument_main}\")\n",
    "                                if return_applied_rules:\n",
    "                                    roles.append({\n",
    "                                        'role': role,\n",
    "                                        'applied_rule': rule_id\n",
    "                                    })\n",
    "                                else:\n",
    "                                    roles.append(role)\n",
    "                                \n",
    "                        if len(roles) > 0: \n",
    "                            predicate_result['arguments'].append({\n",
    "                                'argument': argument,\n",
    "                                'argument_main': argument_main,\n",
    "                                'argument_morph': get_morph(argument_main),\n",
    "                                'argument_analyzed': argument_word,\n",
    "                                'argument_tokens': forward_map[argument],\n",
    "                                'roles': tuple(roles)\n",
    "                            })\n",
    "                    if len(predicate_result['arguments']) > 0:\n",
    "                        predicate_result['arguments'] = tuple(predicate_result['arguments'])\n",
    "                        group_result.append(predicate_result)\n",
    "            result.append({'group': group_name, 'parses': group_result})\n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstraintEnforcer:\n",
    "    def __init__(self, constraints=None):\n",
    "        if constraints is None:\n",
    "            constraints = list()\n",
    "            \n",
    "        self.constraints = constraints\n",
    "        \n",
    "    def add(self, constraint):\n",
    "        self.constraints.append(constraint)\n",
    "        \n",
    "    def enforce(self, parse):\n",
    "        a_parse = parse.copy()\n",
    "        for constraint in self.constraints:\n",
    "            a_parse = constraint(a_parse)\n",
    "            if len(a_parse) == 0:\n",
    "                return a_parse\n",
    "            \n",
    "        return a_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "from slovnet import Syntax\n",
    "navec = Navec.load('data/models/navec_news_v1_1B_250K_300d_100q.tar')\n",
    "syntax = Syntax.load('data/models/slovnet_syntax_news_v1.tar')\n",
    "_ = syntax.navec(navec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_phrase_extractor = MainPhraseExtractor(syntax, MorphAnalyzer())\n",
    "extractor = PredPattArgumentExtractor(\"./data/models/russian-syntagrus-ud-2.5-191206.udpipe\")\n",
    "clause_extractor = RstClauseSeparator(cache_path=\"./experiments/rst-cache.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = RoleLabeler(extractor, main_phrase_extractor, filter_pipeline, predicate_orient_rules, mode='soft', extend_arguments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "enforcer = ConstraintEnforcer()\n",
    "def enforce_parseable_predicate(parse):\n",
    "    if check_parseable(parse['predicate_analyzed'], filter_pipeline):\n",
    "        return parse\n",
    "    else:\n",
    "        return {}\n",
    "    \n",
    "def reduce_duplicate_roles(parse):\n",
    "    new_args = []\n",
    "    for arg in parse['arguments']:\n",
    "        arg['roles'] = tuple(set(arg['roles']))\n",
    "        new_args.append(arg)\n",
    "    parse['arguments'] = new_args\n",
    "    return parse\n",
    "    \n",
    "def resolve_multiple_expirirencers(parse):\n",
    "    if len(parse['arguments']) >= 2:\n",
    "        parse_roles = set(arg['roles'] for arg in parse['arguments'])\n",
    "        if ('экспериенцер',) in parse_roles:\n",
    "            new_args = []\n",
    "            for arg in parse['arguments']:\n",
    "                if len(arg['roles']) >= 2:\n",
    "                    new_roles = list(arg['roles'])\n",
    "                    if 'экспериенцер' in new_roles:\n",
    "                        new_roles.remove('экспериенцер')\n",
    "                    arg['roles'] = tuple(new_roles)\n",
    "                new_args.append(arg)\n",
    "            parse['arguments'] = new_args\n",
    "    return parse\n",
    "\n",
    "def resolve_single_expiriencer(parse):\n",
    "    parse_roles = [arg['roles'] for arg in parse['arguments'] if len(arg['roles']) >= 2]\n",
    "    if len(parse_roles) > 0:\n",
    "        n_exp = 0\n",
    "        for role in parse_roles:\n",
    "            if 'экспериенцер' in role:\n",
    "                n_exp += 1\n",
    "                \n",
    "        if n_exp == 1:\n",
    "            new_args = []\n",
    "            for arg in parse['arguments']:\n",
    "                if len(arg['roles']) >= 2 and 'экспериенцер' in arg['roles']:\n",
    "                    arg['roles'] = ('экспериенцер',)\n",
    "                new_args.append(arg)\n",
    "            parse['arguments'] = new_args\n",
    "    return parse\n",
    "        \n",
    "enforcer.add(enforce_parseable_predicate)\n",
    "enforcer.add(reduce_duplicate_roles) # ('каузатор', 'экспериенцер', 'экспериенцер') ->  ('каузатор', 'экспериенцер')\n",
    "enforcer.add(resolve_multiple_expirirencers)\n",
    "enforcer.add(resolve_single_expiriencer) # ('каузатор', 'экспериенцер') -> ('экспериенцер')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"./data/youtube/youtube_1yeat.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>У нас на всех ток шоу обсуждают всё кроме реал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>Не волнуйся! Все будет хорошо!!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>А есле полезити занимать кредит у МВФ мерового...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>@Александр Чемезов Я знаю и помню, когда не за...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5S_deYcRqI</td>\n",
       "      <td>Глупость! Умный истеблишмент США стравливает п...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file                                               text\n",
       "0  D5S_deYcRqI  У нас на всех ток шоу обсуждают всё кроме реал...\n",
       "1  D5S_deYcRqI                  Не волнуйся! Все будет хорошо!!\\n\n",
       "2  D5S_deYcRqI  А есле полезити занимать кредит у МВФ мерового...\n",
       "3  D5S_deYcRqI  @Александр Чемезов Я знаю и помню, когда не за...\n",
       "4  D5S_deYcRqI  Глупость! Умный истеблишмент США стравливает п..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_punct(target: str, symbol: str = \";\"):\n",
    "    return re.sub(\n",
    "        r'{{(.*)}}'\n",
    "        symbol,\n",
    "        emoji.demojize(target, delimiters=(\"{{\", \"}}\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_apply(series, func, n_cores=4):\n",
    "    pool = Pool(n_cores)\n",
    "    series = pool.map(func, tqdm(series))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005fb9123ba64b269ccf0d880927e053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=596160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parallel_processed = parallel_apply(data['text'], emoji_to_punct, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = parallel_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(596160, 2)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e2ea09c22d433398cc72092c7870bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parses = []\n",
    "empty_parses = []\n",
    "errors = []\n",
    "subdata = data[:100000]\n",
    "for row in tqdm(subdata.itertuples(), total=len(subdata)):\n",
    "    text = row.text\n",
    "    file = row.file\n",
    "    \n",
    "    sentences = [x.text for x in razdel.sentenize(text)]\n",
    "    predicates = len(list(filter_pipeline.findall(text)))\n",
    "    roles = []\n",
    "    clauses = []\n",
    "    n_tokens = 0\n",
    "    for s in sentences:\n",
    "        try:\n",
    "            clauses += clause_extractor.extract(s.replace(\". \", \"\"))\n",
    "        except Exception as e:\n",
    "            errors.append((s, e))\n",
    "        n_tokens += len(list(razdel.tokenize(s)))\n",
    "        \n",
    "    for i, clause in enumerate(clauses):\n",
    "        if check_parseable(clause, filter_pipeline):\n",
    "            groups = labeler.run(clause)\n",
    "            if len(groups) != 0:\n",
    "                for j, group in enumerate(groups):\n",
    "                    for k, parse in enumerate(group['parses']):\n",
    "                        parse = enforcer.enforce(parse)\n",
    "                        if len(parse) != 0:\n",
    "                            parses.append({\n",
    "                                'text': text,\n",
    "                                'clause_text': clause,\n",
    "                                'content_hash': file,\n",
    "                                'clause_idx': i,\n",
    "                                'group_idx': j,\n",
    "                                'parse_idx': k,\n",
    "                                'parse': parse\n",
    "                            })\n",
    "                        else:\n",
    "                            empty_parses.append({\n",
    "                                'text': text,\n",
    "                                'clause_text': clause,\n",
    "                                'content_hash': file,\n",
    "                                'clause_idx': i,\n",
    "                                'group_idx': j,\n",
    "                                'parse_idx': k,\n",
    "                                'parse': parse\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '@Папина Дочка а зачем смотреть, что там увидит, он знает - дебила, зачем лишний раз расстраивать себя, хоть и не умного, но все же любимого.\\n',\n",
       "  'clause_text': '@Папина Дочка а зачем смотреть, что там увидит, он знает - дебила, зачем лишний раз расстраивать себя, хоть и не умного, но все же любимого.',\n",
       "  'content_hash': 'XUOilPRZHDA',\n",
       "  'clause_idx': 0,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'зачем расстраивать',\n",
       "   'predicate_analyzed': 'расстраивать',\n",
       "   'predicate_morph': 'ИНФ,несов,перех',\n",
       "   'predicate_tokens': [зачем/15, расстраивать/18],\n",
       "   'arguments': [{'argument': 'лишний раз',\n",
       "     'argument_main': 'дебила , лишний раз',\n",
       "     'argument_morph': 'СУЩ,неод,жр мн,рд',\n",
       "     'argument_analyzed': 'лишний раз',\n",
       "     'argument_tokens': [лишний/16, раз/17],\n",
       "     'roles': ('экспериенцер',)},\n",
       "    {'argument': 'себя , хоть и не умного',\n",
       "     'argument_main': 'лишний раз себя',\n",
       "     'argument_morph': 'ДЕЕПР,сов,перех прош',\n",
       "     'argument_analyzed': 'себя',\n",
       "     'argument_tokens': [себя/19, ,/20, хоть/21, и/22, не/23, умного/24],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': '@Алекс Ком какой народ не смешить? Народ Украины? Да над вами весь мир смеётся ;\\n',\n",
       "  'clause_text': '@Алекс Ком какой народ не смешить?',\n",
       "  'content_hash': 'XUOilPRZHDA',\n",
       "  'clause_idx': 0,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'не смешить',\n",
       "   'predicate_analyzed': 'смешить',\n",
       "   'predicate_morph': 'ИНФ,несов,перех',\n",
       "   'predicate_tokens': [не/4, смешить/5],\n",
       "   'arguments': [{'argument': 'какой народ',\n",
       "     'argument_main': '@ Алекс какой народ',\n",
       "     'argument_morph': 'СУЩ,неод,мр ед,им',\n",
       "     'argument_analyzed': 'какой народ',\n",
       "     'argument_tokens': [какой/2, народ/3],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': '@Алексей Косинцев Может ты и прав . Крепостным без разницы , кто их будет пороть , унижать , насиловать , убивать и заставлять работать на своё благо .\\n',\n",
       "  'clause_text': 'Крепостным без разницы , кто их будет пороть , унижать , насиловать , убивать и заставлять работать на своё благо .',\n",
       "  'content_hash': 'XUOilPRZHDA',\n",
       "  'clause_idx': 1,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'унижать',\n",
       "   'predicate_analyzed': 'унижать',\n",
       "   'predicate_morph': 'ИНФ,несов,перех',\n",
       "   'predicate_tokens': [унижать/9],\n",
       "   'arguments': [{'argument': 'кто',\n",
       "     'argument_main': 'разницы , кто',\n",
       "     'argument_morph': 'Н',\n",
       "     'argument_analyzed': 'кто',\n",
       "     'argument_tokens': [кто/4],\n",
       "     'roles': ('каузатор',)}]}},\n",
       " {'text': 'Ага,Путина обманешь!!!Держите карман шире,пусть не очаровываются,чтоб потом - не разочароваться!!Путин УМ И СИЛА в одном лице!!\\n',\n",
       "  'clause_text': 'Ага,Путина обманешь!!!Держите карман шире,пусть не очаровываются,чтоб потом - не разочароваться!!Путин УМ И СИЛА в одном лице!!',\n",
       "  'content_hash': 'XUOilPRZHDA',\n",
       "  'clause_idx': 0,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'потом - не разочароваться',\n",
       "   'predicate_analyzed': 'разочароваться',\n",
       "   'predicate_morph': 'ИНФ,сов,неперех',\n",
       "   'predicate_tokens': [потом/16, -/17, не/18, разочароваться/19],\n",
       "   'arguments': [{'argument': 'Путин УМ',\n",
       "     'argument_main': 'УМ И Путин',\n",
       "     'argument_morph': 'СУЩ,од,мр,sg,фам ед,им',\n",
       "     'argument_analyzed': 'Путин',\n",
       "     'argument_tokens': [Путин/22, УМ/23],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': '@Сергей Васильев аккуратнее в выражениях,зачем же себя так унижать,вы кто ? Правильно никто, а Путин президент очень большой ядерной Державы\\n',\n",
       "  'clause_text': '@Сергей Васильев аккуратнее в выражениях,зачем же себя так унижать,вы кто ?',\n",
       "  'content_hash': 'XUOilPRZHDA',\n",
       "  'clause_idx': 0,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'зачем же так унижать',\n",
       "   'predicate_analyzed': 'зачем же так унижать',\n",
       "   'predicate_morph': 'ИНФ,несов,перех',\n",
       "   'predicate_tokens': [зачем/6, же/7, так/9, унижать/10],\n",
       "   'arguments': [{'argument': 'себя',\n",
       "     'argument_main': ', зачем себя',\n",
       "     'argument_morph': 'ДЕЕПР,сов,перех прош',\n",
       "     'argument_analyzed': 'себя',\n",
       "     'argument_tokens': [себя/8],\n",
       "     'roles': ('экспериенцер',)},\n",
       "    {'argument': 'кто',\n",
       "     'argument_main': 'так унижать кто',\n",
       "     'argument_morph': 'Н',\n",
       "     'argument_analyzed': 'кто',\n",
       "     'argument_tokens': [кто/12],\n",
       "     'roles': ('каузатор',)}]}},\n",
       " {'text': '@Lubov Pakhotina Люба , Люба . Я ж тебе уже говорил что не надо на меня натягивать свое извращенное восприятие . так же я тебе довольно прозрачно намекал что способом ведения беседы в стиле \"сам дурак\" тебе меня впечатлить не удастся . Что ж ты такая убогонькая то ? почему не понимаешь русского языка ?\\n',\n",
       "  'clause_text': 'Я ж тебе уже говорил что не надо на меня натягивать свое извращенное восприятие так же я тебе довольно прозрачно намекал что способом ведения беседы в стиле \"сам дурак\" тебе меня впечатлить не удастся .',\n",
       "  'content_hash': 'EXwu-DSoUr8',\n",
       "  'clause_idx': 1,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'впечатлить',\n",
       "   'predicate_analyzed': 'впечатлить',\n",
       "   'predicate_morph': 'ИНФ,сов,перех',\n",
       "   'predicate_tokens': [впечатлить/33],\n",
       "   'arguments': [{'argument': 'меня',\n",
       "     'argument_main': '\" тебе меня',\n",
       "     'argument_morph': 'ДЕЕПР,сов,перех прош,*несов',\n",
       "     'argument_analyzed': 'меня',\n",
       "     'argument_tokens': [меня/32],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': '@Lubov Pakhotina Любаша не надо демонстрировать публично свой уровень развития и словно в детском саду заниматься дискуссией в стиле \"сам такой\"\" Лично я уже все понял о твоих умственных способностях . Так что поразить тебе меня явно не удастся .\\n',\n",
       "  'clause_text': 'Так что поразить тебе меня явно не удастся .',\n",
       "  'content_hash': 'EXwu-DSoUr8',\n",
       "  'clause_idx': 1,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'поразить',\n",
       "   'predicate_analyzed': 'поразить',\n",
       "   'predicate_morph': 'ИНФ,сов,перех',\n",
       "   'predicate_tokens': [поразить/2],\n",
       "   'arguments': [{'argument': 'тебе',\n",
       "     'argument_main': 'что поразить тебе',\n",
       "     'argument_morph': 'СУЩ,неод,мр,орг ед,пр',\n",
       "     'argument_analyzed': 'тебе',\n",
       "     'argument_tokens': [тебе/3],\n",
       "     'roles': ('каузатор',)},\n",
       "    {'argument': 'меня',\n",
       "     'argument_main': 'поразить тебе меня',\n",
       "     'argument_morph': 'ДЕЕПР,сов,перех прош,*несов',\n",
       "     'argument_analyzed': 'меня',\n",
       "     'argument_tokens': [меня/4],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': 'Моё мнение, что они не \"решили не жестить\", у них просто не хватило ресурсов, чтобы жестить. И все это благодаря тому, что акции проходили одновременно по всей стране, поэтому не было возможности сконцентрироваться силовиков в одном месте, в СПб или в Мск, например, как это обычно бывает. И второе, людей вышло действительно много, притом они постоянно двигались, что почти никак не удавалось сдержать и занять какие-то стратегические позиции. Избиения и задержания фактически возможны только в стационарной точке, они их отрабатывают. Так что это из оперы \"хотели бы, отравили\". Хотели-то хотели, да не смогли. Поэтому важно, чтобы протест был распределен по стране, а не концентрироваться в столицах. Ну и массовость, её снижать нельзя, только повышать.\\n',\n",
       "  'clause_text': 'И второе, людей вышло действительно много, притом они постоянно двигались, что почти никак не удавалось сдержать и занять какие-то стратегические позиции.',\n",
       "  'content_hash': 'f3l2ddE9u_8',\n",
       "  'clause_idx': 2,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'занять',\n",
       "   'predicate_analyzed': 'занять',\n",
       "   'predicate_morph': 'ИНФ,сов,перех',\n",
       "   'predicate_tokens': [занять/20],\n",
       "   'arguments': [{'argument': 'какие-то стратегические позиции',\n",
       "     'argument_main': 'какие-то стратегические позиции',\n",
       "     'argument_morph': 'СУЩ,неод,жр ед,рд',\n",
       "     'argument_analyzed': 'позиции',\n",
       "     'argument_tokens': [какие-то/21, стратегические/22, позиции/23],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': 'путину надо все время задавать вопросы про этот дворец и раздражать его\\n',\n",
       "  'clause_text': 'путину надо все время задавать вопросы про этот дворец и раздражать его',\n",
       "  'content_hash': 'f3l2ddE9u_8',\n",
       "  'clause_idx': 0,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'раздражать',\n",
       "   'predicate_analyzed': 'раздражать',\n",
       "   'predicate_morph': 'ИНФ,несов,перех',\n",
       "   'predicate_tokens': [раздражать/10],\n",
       "   'arguments': [{'argument': 'его',\n",
       "     'argument_main': 'и раздражать его',\n",
       "     'argument_morph': 'ПРИЛ мр,ед,рд',\n",
       "     'argument_analyzed': 'его',\n",
       "     'argument_tokens': [его/11],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': 'Поражает не только безумная роскошь Дворца бывшего двоечника. Шок от того, какие хитрые схемы используются для финансирования этого безобразия, сколько людей в этом участвуют и молчат при этом! Что удивляться тогда доносам в сталинское время!\\n',\n",
       "  'clause_text': 'Что удивляться тогда доносам в сталинское время!',\n",
       "  'content_hash': 'f3l2ddE9u_8',\n",
       "  'clause_idx': 2,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'удивляться тогда в',\n",
       "   'predicate_analyzed': 'удивляться',\n",
       "   'predicate_morph': 'ИНФ,несов,неперех',\n",
       "   'predicate_tokens': [удивляться/1, тогда/2, в/4],\n",
       "   'arguments': [{'argument': 'доносам',\n",
       "     'argument_main': 'удивляться тогда доносам',\n",
       "     'argument_morph': 'СУЩ,неод,мр мн,дт',\n",
       "     'argument_analyzed': 'доносам',\n",
       "     'argument_tokens': [доносам/3],\n",
       "     'roles': ('каузатор',)}]}},\n",
       " {'text': '@Ярослав Ефремов Live CH волноваться нечему, состояние стабильное к счастью\\n',\n",
       "  'clause_text': '@Ярослав Ефремов Live CH волноваться нечему, состояние стабильное к счастью',\n",
       "  'content_hash': '6Fyu2Ooz-sg',\n",
       "  'clause_idx': 0,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'волноваться',\n",
       "   'predicate_analyzed': 'волноваться',\n",
       "   'predicate_morph': 'ИНФ,несов,неперех',\n",
       "   'predicate_tokens': [волноваться/4],\n",
       "   'arguments': [{'argument': 'нечему',\n",
       "     'argument_main': 'Live CH нечему',\n",
       "     'argument_morph': 'ПРИЛ мр,ед,дт',\n",
       "     'argument_analyzed': 'нечему',\n",
       "     'argument_tokens': [нечему/5],\n",
       "     'roles': ('каузатор',)}]}},\n",
       " {'text': '@Евгений Поляков нечего сказать? Тогда лучше умных людей послушай, не показывай свою дурь, оскорблять оппонента, себя позорить\\n',\n",
       "  'clause_text': 'Тогда лучше умных людей послушай, не показывай свою дурь, оскорблять оппонента, себя позорить',\n",
       "  'content_hash': '6Fyu2Ooz-sg',\n",
       "  'clause_idx': 1,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'оскорблять',\n",
       "   'predicate_analyzed': 'оскорблять',\n",
       "   'predicate_morph': 'ИНФ,несов,перех',\n",
       "   'predicate_tokens': [оскорблять/11],\n",
       "   'arguments': [{'argument': 'оппонента',\n",
       "     'argument_main': ', оскорблять оппонента',\n",
       "     'argument_morph': 'СУЩ,неод,мр ед,рд',\n",
       "     'argument_analyzed': 'оппонента',\n",
       "     'argument_tokens': [оппонента/12],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': 'Ну видно же, что человек не совсем так скажем умный( чтоб не оскорбить чиновника)...так зачем он хабаровчанам? Это им, как плевок в лицо!\\n',\n",
       "  'clause_text': 'Ну видно же, что человек не совсем так скажем умный( чтоб не оскорбить чиновника)...так зачем он хабаровчанам?',\n",
       "  'content_hash': '6Fyu2Ooz-sg',\n",
       "  'clause_idx': 0,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'не оскорбить',\n",
       "   'predicate_analyzed': 'оскорбить',\n",
       "   'predicate_morph': 'ИНФ,сов,перех',\n",
       "   'predicate_tokens': [не/13, оскорбить/14],\n",
       "   'arguments': [{'argument': 'человек',\n",
       "     'argument_main': ', что человек',\n",
       "     'argument_morph': 'СУЩ,од,мр ед,им',\n",
       "     'argument_analyzed': 'человек',\n",
       "     'argument_tokens': [человек/5],\n",
       "     'roles': ('каузатор',)},\n",
       "    {'argument': 'чиновника',\n",
       "     'argument_main': 'не оскорбить чиновника',\n",
       "     'argument_morph': 'СУЩ,неод,мр ед,рд',\n",
       "     'argument_analyzed': 'чиновника',\n",
       "     'argument_tokens': [чиновника/15],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': 'Он просто работает, зачем его обижать, служба, точнее прислужба, у него такая)\\n',\n",
       "  'clause_text': 'Он просто работает, зачем его обижать, служба, точнее прислужба, у него такая)',\n",
       "  'content_hash': '6Fyu2Ooz-sg',\n",
       "  'clause_idx': 0,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'зачем обижать',\n",
       "   'predicate_analyzed': 'обижать',\n",
       "   'predicate_morph': 'ИНФ,несов,перех',\n",
       "   'predicate_tokens': [зачем/4, обижать/6],\n",
       "   'arguments': [{'argument': 'его',\n",
       "     'argument_main': ', зачем его',\n",
       "     'argument_morph': 'ПРИЛ мр,ед,рд',\n",
       "     'argument_analyzed': 'его',\n",
       "     'argument_tokens': [его/5],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': 'Прислали Дегтярева специально,чтобы раздражать людей.Как можно это спокойно вынести?\\n',\n",
       "  'clause_text': 'Прислали Дегтярева специально,чтобы раздражать людей.Как можно это спокойно вынести?',\n",
       "  'content_hash': '6Fyu2Ooz-sg',\n",
       "  'clause_idx': 0,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'раздражать',\n",
       "   'predicate_analyzed': 'раздражать',\n",
       "   'predicate_morph': 'ИНФ,несов,перех',\n",
       "   'predicate_tokens': [раздражать/5],\n",
       "   'arguments': [{'argument': 'людей',\n",
       "     'argument_main': 'чтобы раздражать людей',\n",
       "     'argument_morph': 'СУЩ,неод,жр мн,рд',\n",
       "     'argument_analyzed': 'людей',\n",
       "     'argument_tokens': [людей/6],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': 'О Боже,как же оподлел человек!И сын и родители!Чему же удивляться, что и врачей сейчас ненавидят,им не доверяют!Но честные и Правда все равно побеждают!Молодец Навальный!Дай,Бог тебе и всей твоей семье здоровья,коль Бог избрал тебя своим Рупором!А своих Творец всегда сохранит и в этой жизни и для будущей!!!\\n',\n",
       "  'clause_text': 'О Боже,как же оподлел человек!И сын и родители!Чему же удивляться, что и врачей сейчас ненавидят,им не доверяют!Но честные и Правда все равно побеждают!Молодец Навальный!Дай,Бог тебе и всей твоей семье здоровья,коль Бог избрал тебя своим Рупором!А своих Творец всегда сохранит и в этой жизни и для будущей!!!',\n",
       "  'content_hash': '6Fyu2Ooz-sg',\n",
       "  'clause_idx': 0,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'удивляться',\n",
       "   'predicate_analyzed': 'удивляться',\n",
       "   'predicate_morph': 'ИНФ,несов,неперех',\n",
       "   'predicate_tokens': [удивляться/15],\n",
       "   'arguments': [{'argument': 'Чему же',\n",
       "     'argument_main': 'родители ! Чему',\n",
       "     'argument_morph': 'ПРИЛ,кач мр,ед,дт',\n",
       "     'argument_analyzed': 'Чему',\n",
       "     'argument_tokens': [Чему/13, же/14],\n",
       "     'roles': ('каузатор',)}]}},\n",
       " {'text': 'Не надо оскорблять Дегтярева. Его послал Путин.\\n',\n",
       "  'clause_text': 'Не надо оскорблять Дегтярева.',\n",
       "  'content_hash': '6Fyu2Ooz-sg',\n",
       "  'clause_idx': 0,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'оскорблять',\n",
       "   'predicate_analyzed': 'оскорблять',\n",
       "   'predicate_morph': 'ИНФ,несов,перех',\n",
       "   'predicate_tokens': [оскорблять/2],\n",
       "   'arguments': [{'argument': 'Дегтярева',\n",
       "     'argument_main': 'надо оскорблять Дегтярева',\n",
       "     'argument_morph': 'СУЩ,од,мр,sg,фам ед,рд',\n",
       "     'argument_analyzed': 'Дегтярева',\n",
       "     'argument_tokens': [Дегтярева/3],\n",
       "     'roles': ('экспериенцер',)}]}},\n",
       " {'text': 'Скажите мне, что он прикидывается ! Нельзя быть настолько тупым и при этом занимать настолько высокие должности\\n',\n",
       "  'clause_text': 'Нельзя быть настолько тупым и при этом занимать настолько высокие должности',\n",
       "  'content_hash': '6Fyu2Ooz-sg',\n",
       "  'clause_idx': 1,\n",
       "  'group_idx': 0,\n",
       "  'parse_idx': 0,\n",
       "  'parse': {'predicate': 'при занимать',\n",
       "   'predicate_analyzed': 'занимать',\n",
       "   'predicate_morph': 'ИНФ,несов,перех',\n",
       "   'predicate_tokens': [при/5, занимать/7],\n",
       "   'arguments': [{'argument': 'настолько высокие должности',\n",
       "     'argument_main': 'настолько высокие должности',\n",
       "     'argument_morph': 'СУЩ,неод,жр ед,рд',\n",
       "     'argument_analyzed': 'должности',\n",
       "     'argument_tokens': [настолько/8, высокие/9, должности/10],\n",
       "     'roles': ('экспериенцер',)}]}}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69111"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_extractor.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/results_youtube_emojipunct.pkl']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump({'full_parses': parses, 'empty_parses': empty_parses}, FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Done\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fbf6872e790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
