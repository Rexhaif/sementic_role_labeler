{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from typing import *\n",
    "\n",
    "import yargy as y\n",
    "import yargy.predicates as yp\n",
    "import yargy.morph as ytm\n",
    "import yargy.tokenizer as yt\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from functools import lru_cache\n",
    "CACHE_SIZE=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostProbMorphAnalyzer(ytm.MorphAnalyzer):\n",
    "\n",
    "    def __call__(self, word):\n",
    "        records = self.raw.parse(word)\n",
    "        max_score = max(x.score for x in records)\n",
    "        records = list(filter(lambda x: x.score == max_score, records))\n",
    "        return [ytm.prepare_form(record) for record in records]\n",
    "    \n",
    "    \n",
    "class CachedMostProbMorphAnalyzer(MostProbMorphAnalyzer):\n",
    "    def __init__(self):\n",
    "        super(CachedMostProbMorphAnalyzer, self).__init__()\n",
    "        \n",
    "    __call__ = lru_cache(CACHE_SIZE)(MostProbMorphAnalyzer.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = pd.read_csv(\"../data/rules/rules_formatted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates_ = pd.read_csv(\"../data/rules/predicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "deverbal_nouns = set(predicates_[predicates_.type == 'deverbal_noun'].predicate.to_list())\n",
    "predicates = set(predicates_[predicates_.type == 'predicate'].predicate.to_list())\n",
    "status_categories = set(predicates_[predicates_.type == 'status_category'].predicate.to_list())\n",
    "rule_specific = set(rules.predicate.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicates = predicates | deverbal_nouns | status_categories | rule_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predicate_rule(\n",
    "    require_deverbal_noun: str,\n",
    "    require_reflexive: str,\n",
    "    require_status_category: str,\n",
    "    predicate: str,\n",
    "    **kwargs\n",
    "):\n",
    "    rule_id = f\"predicate={predicate},deverbal={require_deverbal_noun},reflexive={require_reflexive},status_category={require_status_category}\"\n",
    "    return rule_id, y.rule(\n",
    "        y.and_(\n",
    "            req_predicate(predicate),\n",
    "            req_deverbal(require_deverbal_noun),\n",
    "            req_reflexive(require_reflexive)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_argument_role(argument_type: str, case: str, preposition: str, **kwargs):\n",
    "    rule_id = f\"argument_type={argument_type},case={case},preposition={preposition}\"\n",
    "    arg = y.and_(\n",
    "        req_argument(),\n",
    "        req_animacy(argument_type),\n",
    "        req_case(case)\n",
    "    )\n",
    "    internal = y.and_(\n",
    "        yp.gram(\"ADJF\"), \n",
    "        y.or_(\n",
    "            yp.normalized(\"этот\"),\n",
    "            yp.normalized(\"тот\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    rule = y.or_(\n",
    "        y.rule(req_preposition(preposition), arg),\n",
    "        y.rule(req_preposition(preposition), internal, arg)\n",
    "    )\n",
    "    return rule_id, rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_deverbal(require_deverbal_noun: str = '?'):\n",
    "    if require_deverbal_noun == '1': ## strictly deverbal noun\n",
    "        return y.and_(\n",
    "            yp.gram(\"NOUN\"),\n",
    "            yp.in_caseless(deverbal_nouns)\n",
    "        )\n",
    "    elif require_deverbal_noun == '0': ## strictly regular verb\n",
    "        return y.or_(\n",
    "            yp.gram(\"VERB\"),\n",
    "            yp.gram(\"INFN\")\n",
    "        )\n",
    "    elif require_deverbal_noun == '?': ## anything\n",
    "        return y.or_(\n",
    "            y.and_(\n",
    "                yp.gram(\"NOUN\"),\n",
    "                yp.in_caseless(deverbal_nouns)\n",
    "            ),\n",
    "            yp.gram(\"VERB\"),\n",
    "            yp.gram(\"INFN\")\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect deverbal status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_reflexive(reflexive_status: str = '?'):\n",
    "    \n",
    "    def is_reflexive_verb(verb: str):\n",
    "        return verb.endswith(\"ся\")\n",
    "    \n",
    "    if reflexive_status == \"1\":\n",
    "        return yp.custom(is_reflexive_verb)\n",
    "    if reflexive_status == \"0\":\n",
    "        return y.not_(yp.custom(is_reflexive_verb))\n",
    "    elif reflexive_status == \"?\":\n",
    "        return yp.true()\n",
    "    else:\n",
    "        raise ValueError (\"Incorrect reflexive status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_animacy(animacy: str = 'любой'):\n",
    "    if animacy == 'любой':\n",
    "        return yp.true()\n",
    "    elif animacy == 'одуш.':\n",
    "        return y.or_(\n",
    "            y.not_(yp.gram('inan')),\n",
    "            yp.gram(\"anim\"),\n",
    "            yp.gram(\"NPRO\"),\n",
    "            yp.gram(\"ADJF\")\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect Animacy Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_argument():\n",
    "    return y.and_(\n",
    "        y.not_(\n",
    "            y.or_(\n",
    "                yp.gram('PREP'),\n",
    "                yp.gram(\"CONJ\"),\n",
    "                yp.gram('PRCL'),\n",
    "                yp.gram(\"INTJ\")\n",
    "            )\n",
    "        ),\n",
    "        y.or_(\n",
    "            yp.gram(\"NOUN\"),\n",
    "            yp.gram(\"NPRO\"),\n",
    "            yp.gram(\"ADJF\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_predicate(word: str = \"?\"):\n",
    "    predicate = y.or_(\n",
    "        yp.gram(\"VERB\"),\n",
    "        yp.gram(\"INFN\"),\n",
    "        yp.gram(\"NOUN\")\n",
    "    )\n",
    "    if word != '?':\n",
    "        predicate = y.and_(\n",
    "            yp.normalized(word),\n",
    "            predicate\n",
    "        )\n",
    "        \n",
    "    return predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_case(case: str = 'в'):\n",
    "    if case == 'в':\n",
    "        pred = yp.gram(\"accs\")\n",
    "    elif case == 'т':\n",
    "        pred = yp.gram(\"ablt\")\n",
    "    elif case == 'д':\n",
    "        pred = yp.gram('datv')\n",
    "    elif case == 'р':\n",
    "        pred = yp.gram(\"gent\")\n",
    "    elif case == 'и':\n",
    "        pred = yp.gram(\"nomn\")\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect Case\")\n",
    "    \n",
    "    return y.or_(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_preposition(preposition: str = None):\n",
    "    if preposition == 'None':\n",
    "        return y.empty()\n",
    "    else:\n",
    "        return y.and_(\n",
    "            yp.gram(\"PREP\"),\n",
    "            yp.eq(preposition)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_parser_pass(parser, text):\n",
    "    matches = []\n",
    "    for match in parser.findall(text):\n",
    "        matches.append({\n",
    "            'text': \" \".join([x.value for x in match.tokens]),\n",
    "            'span': tuple(match.span)\n",
    "        })\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_parser_pass(parser, text):\n",
    "    match = parser.match(text)\n",
    "    matches.append({\n",
    "        'text': \" \".join([x.value for x in match.tokens]),\n",
    "        'span': tuple(match.span)\n",
    "    })\n",
    "\n",
    "    return [match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rules(**kwargs):\n",
    "    predicate_rule_id, predicate_rule = create_predicate_rule(**kwargs)\n",
    "    argument_rule_id, argument_rule = create_argument_role(**kwargs)\n",
    "    return {\n",
    "        'predicate_id': predicate_rule_id,\n",
    "        'argument_id': argument_rule_id,\n",
    "        'predicate_parser': y.Parser(predicate_rule, yt.MorphTokenizer(morph=CachedMostProbMorphAnalyzer())),\n",
    "        'argument_parser': y.Parser(argument_rule, yt.MorphTokenizer(morph=CachedMostProbMorphAnalyzer()))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleset = set(rules.role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3424a029d1444a5870c8ccad6f25be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='инструмент'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b4211a5bf44f10b63fec387f15aaa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='каузатив'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcae85e0c124d2cbe3dcdaf989b88ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='объект'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62694c65ac5542d6b87f698fc29ea974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='каузатор'), FloatProgress(value=0.0, max=31.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69da73f2d9f541dc9d65349f32143d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='экспериенцер'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for role in roleset:\n",
    "    ruleset[role] = []\n",
    "    \n",
    "    for rule_dict in tqdm(rules[rules.role == role].to_dict(orient='records'), desc=role):\n",
    "        ruleset[role].append(create_rules(**rule_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument_rules = {}\n",
    "for role in ruleset.keys():\n",
    "    for rule in ruleset[role]:\n",
    "        argument_rules[f\"{rule['argument_id']}+{role}\"] = {\n",
    "            'role':role,\n",
    "            'argument_parser': rule['argument_parser']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_orient = {}\n",
    "for role in ruleset.keys():\n",
    "    for rule in ruleset[role]:\n",
    "        rule_id = rule['predicate_id']\n",
    "        if rule_id not in predicate_orient:\n",
    "            predicate_orient[rule_id] = {}\n",
    "            predicate_orient[rule_id]['predicate_parser'] = rule['predicate_parser']\n",
    "            predicate_orient[rule_id]['arguments'] = []\n",
    "            \n",
    "        predicate_orient[rule_id]['arguments'].append({\n",
    "            'role': role,\n",
    "            'argument_id': rule['argument_id'],\n",
    "            'argument_parser': rule['argument_parser']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_orient_rules = {}\n",
    "for predicate_id in predicate_orient.keys():\n",
    "    argument_tuples = set([\n",
    "        f\"{x['argument_id']}+{x['role']}\" for x in predicate_orient[predicate_id]['arguments']\n",
    "    ])\n",
    "    predicate_orient_rules[predicate_id] = {\n",
    "        'predicate_parser': predicate_orient[predicate_id]['predicate_parser'],\n",
    "        'arguments': [argument_rules[key] for key in argument_tuples]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yargy.pipelines as pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pipeline = y.Parser(\n",
    "    pipelines.morph_pipeline(list(all_predicates)),\n",
    "    tokenizer=yt.MorphTokenizer(\n",
    "        morph=CachedMostProbMorphAnalyzer()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parseable(text, parser):\n",
    "    return len(list(parser.findall(text))) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_parseable(\"Вашингтон возмущается бездействием Москвы\", filter_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_parseable(\"взволнованные\", filter_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgumentExtractor:\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def extract(self, sentence: str) -> List[Dict[str, Any]]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline, ProcessingError\n",
    "from predpatt import PredPatt, load_conllu\n",
    "from predpatt import PredPattOpts\n",
    "from predpatt.util.ud import dep_v1, dep_v2\n",
    "\n",
    "class PredPattArgumentExtractor(ArgumentExtractor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_to_udpipe: str,\n",
    "        resolve_relcl: bool = True,\n",
    "        resolve_appos: bool = True,\n",
    "        resolve_amod: bool = True,\n",
    "        resolve_conj: bool = True,\n",
    "        resolve_poss: bool = True,\n",
    "        ud = dep_v2.VERSION\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = Model.load(path_to_udpipe)\n",
    "        self.pipeline = Pipeline(self.model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "        self._error = ProcessingError()\n",
    "        self._opts = PredPattOpts(\n",
    "            resolve_relcl=resolve_relcl,\n",
    "            resolve_appos=resolve_appos,\n",
    "            resolve_amod=resolve_amod,\n",
    "            resolve_conj=resolve_conj,\n",
    "            resolve_poss=resolve_poss,\n",
    "            ud=ud\n",
    "        )\n",
    "        \n",
    "    def extract(self, sentence: str) -> List[Dict[str, Any]]:\n",
    "        processed = self.pipeline.process(sentence, self._error)\n",
    "        if self._error.occurred():\n",
    "            print(f\"=== Error occurred: {self._error.message}\")\n",
    "            self._error = ProcessingError()\n",
    "            return None\n",
    "        else:\n",
    "            conll_example = [ud_parse for sent_id, ud_parse in load_conllu(processed)][0]\n",
    "            ppatt = PredPatt(conll_example, opts=self._opts)\n",
    "            result = []\n",
    "            for predicate in ppatt.instances:\n",
    "                structure = {\n",
    "                    'predicate': predicate.tokens,\n",
    "                    'arguments': [x.tokens for x in predicate.arguments]\n",
    "                }\n",
    "                result.append(structure)\n",
    "                \n",
    "            return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = PredPattArgumentExtractor(\"../data/models/russian-syntagrus-ud-2.5-191206.udpipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.33 ms ± 202 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 7\n",
    "check_parseable('не опечалься на в твоем', filter_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "from slovnet import Syntax\n",
    "navec = Navec.load('../data/models/navec_news_v1_1B_250K_300d_100q.tar')\n",
    "syntax = Syntax.load('../data/models/slovnet_syntax_news_v1.tar')\n",
    "_ = syntax.navec(navec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = next(syntax.map([['продолжает','радовать']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SyntaxToken(\n",
       "     id='1',\n",
       "     text='продолжает',\n",
       "     head_id='0',\n",
       "     rel='root'\n",
       " ),\n",
       " SyntaxToken(\n",
       "     id='2',\n",
       "     text='радовать',\n",
       "     head_id='1',\n",
       "     rel='xcomp'\n",
       " )]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "extr = MainPhraseExtractor(syntax, MorphAnalyzer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SyntaxToken(\n",
       "     id='1',\n",
       "     text='бездействием',\n",
       "     head_id='0',\n",
       "     rel='root'\n",
       " )]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extr.get_main_phrase(['бездействием','Москвы'], get_prep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class MainPhraseExtractor:\n",
    "    \n",
    "    def __init__(self, syntax_parser, pymorphy_analyzer):\n",
    "        self.syntax = syntax_parser\n",
    "        self.morph = pymorphy_analyzer\n",
    "\n",
    "    def get_main_phrase(self, words, get_prep=False, verbose=False):\n",
    "        markup = next(self.syntax.map([words]))\n",
    "        forward = {}\n",
    "        backward = defaultdict(list)\n",
    "        token_map = {}\n",
    "        candidates = []\n",
    "        for token in markup.tokens:\n",
    "            if token.head_id not in backward:\n",
    "                backward[token.head_id] = []\n",
    "\n",
    "            token_map[token.id] = token\n",
    "            forward[token.id] = token.head_id\n",
    "            backward[token.head_id].append(token.id)\n",
    "\n",
    "            if token.id == token.head_id or token.head_id == '0':\n",
    "                candidates.append(token.id)\n",
    "             \n",
    "        if verbose:\n",
    "            print(\"forward \", forward)\n",
    "            print(\"backward \", backward)\n",
    "            print(\"candidates \", candidates)\n",
    "                \n",
    "        if len(candidates) == 0:\n",
    "            return markup.tokens\n",
    "\n",
    "        candidate = sorted(candidates, key=lambda x: len(backward[x]))[-1]\n",
    "        if get_prep:\n",
    "            prep_candidates = backward[candidate]\n",
    "            prep_candidates = list(\n",
    "                filter(lambda x: self.morph.tag(token_map[x].text)[0].POS == 'PREP', prep_candidates)\n",
    "            )\n",
    "            if len(prep_candidates) == 0:\n",
    "                return [token_map[candidate]]\n",
    "            \n",
    "            prep = sorted(prep_candidates, key=lambda x: abs(int(x) - int(candidate)))[0]\n",
    "            return (token_map[prep], token_map[candidate])\n",
    "\n",
    "        return [token_map[candidate]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoleLabeler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        argument_extractor: ArgumentExtractor,\n",
    "        main_phrase_extractor: MainPhraseExtractor,\n",
    "        filter_pipeline,\n",
    "        predicate_ruleset,\n",
    "        mode: str = 'soft'\n",
    "    ):\n",
    "        \n",
    "        self.argument_extractor = argument_extractor\n",
    "        self.main_phrase_extractor = main_phrase_extractor\n",
    "        self.filter_pipeline = filter_pipeline\n",
    "        self.ruleset = predicate_ruleset\n",
    "        if mode == 'soft':\n",
    "            self.pass_fn = soft_parser_pass\n",
    "        elif mode == 'strict':\n",
    "            self.pass_fn = strict_parser_pass\n",
    "        else:\n",
    "            raise ValueError(f\"Incorrect mode = {mode}, can be 'soft' or 'strict'\")\n",
    "            \n",
    "    def check_parse(self, text, parser):\n",
    "        return len(self.pass_fn(parser, text)) > 0\n",
    "    \n",
    "    def run(self, sentence):\n",
    "        arg_groups = self.argument_extractor.extract(sentence)\n",
    "        arg_groups = list(\n",
    "            filter(\n",
    "                lambda x: check_parseable(\n",
    "                    \" \".join([token.text for token in x['predicate']]),\n",
    "                    self.filter_pipeline\n",
    "                ),\n",
    "                arg_groups\n",
    "            )\n",
    "        )\n",
    "        result = []\n",
    "        for group in arg_groups:\n",
    "            \n",
    "            predicate_txt = \" \".join([token.text for token in group['predicate']])\n",
    "            predicate_tokens = [token.text for token in group['predicate']]\n",
    "            predicate_main = \" \".join([x.text for x in self.main_phrase_extractor.get_main_phrase(predicate_tokens)])\n",
    "            forward_map = {\" \".join([token.text for token in argument]): argument for argument in group['arguments']}\n",
    "            group_name = f\"predicate={predicate_txt},arguments=[{','.join(forward_map.keys())}]\"\n",
    "            group_result = []\n",
    "            for predicate in self.ruleset.values():\n",
    "                if self.check_parse(predicate_main, predicate['predicate_parser']):\n",
    "                    predicate_result = {\n",
    "                        'predicate': predicate_txt,\n",
    "                        'predicate_analyzed': predicate_main,\n",
    "                        'predicate_tokens': group['predicate'],\n",
    "                        'arguments': []\n",
    "                    }\n",
    "                    for argument in forward_map.keys():\n",
    "                        argument_tokens = [x.text for x in forward_map[argument]]\n",
    "                        argument_main = \" \".join([\n",
    "                            x.text for x in self.main_phrase_extractor.get_main_phrase(argument_tokens, True)\n",
    "                        ])\n",
    "                        roles = [\n",
    "                            rule['role'] for rule in predicate['arguments'] \n",
    "                            if self.check_parse(argument_main, rule['argument_parser'])\n",
    "                        ]\n",
    "                        if len(roles) > 0: \n",
    "                            predicate_result['arguments'].append({\n",
    "                                'argument': argument,\n",
    "                                'argument_analyzed': argument_main,\n",
    "                                'argument_tokens': forward_map[argument],\n",
    "                                'roles': tuple(roles)\n",
    "                            })\n",
    "                    if len(predicate_result['arguments']) > 0:\n",
    "                        predicate_result['arguments'] = tuple(predicate_result['arguments'])\n",
    "                        group_result.append(predicate_result)\n",
    "            result.append({'group': group_name, 'parses': group_result})\n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_phrase_extractor = MainPhraseExtractor(syntax, MorphAnalyzer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = RoleLabeler(extractor, main_phrase_extractor, filter_pipeline, predicate_orient_rules, mode='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.42 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 7\n",
    "extractor.extract(\"Вашингтон возмущается бездействием Москвы\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'group': 'predicate=возмущается,arguments=[Вашингтон,бездействием Москвы]',\n",
       "  'parses': [{'predicate': 'возмущается',\n",
       "    'predicate_analyzed': 'возмущается',\n",
       "    'predicate_tokens': [возмущается/1],\n",
       "    'arguments': ({'argument': 'бездействием Москвы',\n",
       "      'argument_analyzed': 'бездействием',\n",
       "      'argument_tokens': [бездействием/2, Москвы/3],\n",
       "      'roles': ('каузатор',)},)}]}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeler.run(\"Вашингтон возмущается бездействием Москвы\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 ms ± 3.3 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 7\n",
    "labeler.run(\"Все остальные более сложные или более сомнительные вещи можно выдавать на факультативных занятиях и кружках , если мелкому будет не интересно.|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(text, parse):\n",
    "    tokens = {i: x.text for i, x in enumerate(razdel.tokenize(text))}\n",
    "    arguments = []\n",
    "    sample_role = None\n",
    "    for i, arg in enumerate(parse['arguments']):\n",
    "        arguments.append(arg['argument_analyzed'])\n",
    "        if i == 0 or sample_role is None:\n",
    "            if len(arg['roles']) > 0:\n",
    "                sample_role = arg['roles'][0]\n",
    "        new_tokens = [x.text for x in arg['argument_tokens']]\n",
    "        new_tokens[0] = f\"[{new_tokens[0]}\"\n",
    "        new_tokens[-1] = f\"{new_tokens[-1]}#{'/'.join(set(arg['roles']))}]\"\n",
    "        new_tokens = {arg['argument_tokens'][i].position: new_tokens[i] for i in range(len(new_tokens))}\n",
    "        tokens = {**tokens, **new_tokens}\n",
    "        \n",
    "    new_tokens = [x.text for x in parse['predicate_tokens']]\n",
    "    new_tokens[0] = f\"[{new_tokens[0]}\"\n",
    "    new_tokens[-1] = f\"{new_tokens[-1]}@Предикат]\"\n",
    "    new_tokens = {parse['predicate_tokens'][i].position: new_tokens[i] for i in range(len(new_tokens))}\n",
    "    tokens = {**tokens, **new_tokens}\n",
    "    tokens = sorted(tokens.items(), key=lambda x: x[0])\n",
    "    return {\n",
    "        'formatted_text': \" \".join([x[1] for x in tokens]),\n",
    "        'orig_text': text,\n",
    "        'predicate': parse['predicate_analyzed'],\n",
    "        'arguments': arguments,\n",
    "        'role': sample_role\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [list(x.iterdir()) for x in Path(\"../data/txts/\").iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(chain.from_iterable(cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(chain.from_iterable([x.iterdir() for x in cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(chain.from_iterable([x.iterdir() for x in cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9429"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2365fc6bc91426eb60ae6b58572a32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9429.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for file in tqdm(files):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        sentences += list(razdel.sentenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [x.text for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a6df81791541d1b0d727bb1c6baa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "parse_res = []\n",
    "for text in tqdm(sentences[:100000]):\n",
    "    groups = labeler.run(text)\n",
    "    for group in groups:\n",
    "        for parse in group['parses']:\n",
    "            try:\n",
    "                parse_res.append(visualize(text, parse))\n",
    "            except Exception:\n",
    "                print(f\"== Error with {text}\")\n",
    "                print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1453"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parse_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'formatted_text': '[Гениальный руководитель#каузатор/экспериенцер] Беглов [продолжает радовать@Предикат] .',\n",
       " 'orig_text': 'Гениальный руководитель Беглов продолжает радовать.',\n",
       " 'predicate': 'продолжает',\n",
       " 'arguments': ['руководитель'],\n",
       " 'role': 'экспериенцер'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag2str(tag):\n",
    "    return \",\".join(sorted(tag.grammemes_cyr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def get_most_prob_tags(word, morph):\n",
    "    variants = morph.parse(word)\n",
    "    max_score = max(x.score for x in variants)\n",
    "    variants = filter(\n",
    "        lambda x: x.score == max_score,\n",
    "        variants\n",
    "    )\n",
    "    variants = list(map(lambda x: x.tag, variants))\n",
    "    return variants\n",
    "\n",
    "def get_morph_string(parse):\n",
    "    predicate = [x.text for x in razdel.tokenize(parse['predicate'])]\n",
    "    arguments = parse['arguments']\n",
    "    arguments = [[x.text for x in razdel.tokenize(arg)] for arg in arguments]\n",
    "    predicate_tags = [get_most_prob_tags(x, morph) for x in predicate]\n",
    "    arguments_tags = [[get_most_prob_tags(x, morph) for x in arg] for arg in arguments]\n",
    "    \n",
    "    arguments_str = \"\"\n",
    "    for i in range(len(arguments)):\n",
    "        words = arguments[i]\n",
    "        for j in range(len(words)):\n",
    "            word = words[j]\n",
    "            tags = arguments_tags[i][j]\n",
    "            tags = list(map(tag2str, tags))\n",
    "            arg_str = f\"{word}#[{'/'.join(tags)}]\"\n",
    "            arguments_str += arg_str + \";\"\n",
    "            \n",
    "    predicate_str = \"\"\n",
    "    for i in range(len(predicate)):\n",
    "        word = predicate[i]\n",
    "        tags = predicate_tags[i]\n",
    "        tags = list(map(tag2str, tags))\n",
    "        predicate_str += f\"{word}#[{'/'.join(tags)}]\"\n",
    "        \n",
    "    result_str = f\"Предикат={predicate_str}||Аргументы={arguments_str}\"\n",
    "        \n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Предикат=продолжает#[3л,ГЛ,ед,изъяв,наст,несов,перех]||Аргументы=руководитель#[СУЩ,ед,им,мр,од];'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_morph_string(parse_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e944252e6834aac996bb214bda346b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1453.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for obj in tqdm(parse_res):\n",
    "    obj['morph'] = get_morph_string(obj)\n",
    "    obj['arguments'] = \";\".join(obj['arguments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'formatted_text': '[Гениальный руководитель#каузатор/экспериенцер] Беглов [продолжает радовать@Предикат] .',\n",
       " 'orig_text': 'Гениальный руководитель Беглов продолжает радовать.',\n",
       " 'predicate': 'продолжает',\n",
       " 'arguments': 'руководитель',\n",
       " 'role': 'экспериенцер',\n",
       " 'morph': 'Предикат=продолжает#[3л,ГЛ,ед,изъяв,наст,несов,перех]||Аргументы=руководитель#[СУЩ,ед,им,мр,од];'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = pd.DataFrame(parse_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse['_pr'] = parse['predicate'] + \"+\" + parse['role']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse.to_csv(\"parsed_roles-100k.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = parse['_pr'].value_counts().to_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = parse.sample(frac=1.0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3342baf3b46348f1b6d291076659385d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=856.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "refined = []\n",
    "for pair in tqdm(pairs):\n",
    "    refined += parse[parse['_pr'] == pair].head(10).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parse = pd.DataFrame(refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parse = new_parse.loc[:, ['role','predicate','arguments','morph', 'formatted_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parse.to_csv(\"./parsed_roles-100k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
