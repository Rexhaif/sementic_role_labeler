{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from typing import *\n",
    "\n",
    "import yargy as y\n",
    "import yargy.predicates as yp\n",
    "import yargy.morph as ytm\n",
    "import yargy.tokenizer as yt\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.processor_mystem import ProcessorMystem\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd \n",
    "\n",
    "from pyhash import city_32\n",
    "import joblib as jb\n",
    "import json\n",
    "\n",
    "from rich import print\n",
    "import razdel\n",
    "\n",
    "import os\n",
    "\n",
    "from functools import lru_cache\n",
    "CACHE_SIZE=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostProbMorphAnalyzer(ytm.MorphAnalyzer):\n",
    "\n",
    "    def __call__(self, word):\n",
    "        records = self.raw.parse(word)\n",
    "        max_score = max(x.score for x in records)\n",
    "        records = list(filter(lambda x: x.score == max_score, records))\n",
    "        return [ytm.prepare_form(record) for record in records]\n",
    "    \n",
    "    \n",
    "class CachedMostProbMorphAnalyzer(MostProbMorphAnalyzer):\n",
    "    def __init__(self):\n",
    "        super(CachedMostProbMorphAnalyzer, self).__init__()\n",
    "        \n",
    "    __call__ = lru_cache(CACHE_SIZE)(MostProbMorphAnalyzer.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = pd.read_csv(\"../data/rules/rules_formatted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates_ = pd.read_csv(\"../data/rules/predicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deverbal_nouns = set(predicates_[predicates_.type == 'deverbal_noun'].predicate.to_list())\n",
    "predicates = set(predicates_[predicates_.type == 'predicate'].predicate.to_list())\n",
    "status_categories = set(predicates_[predicates_.type == 'status_category'].predicate.to_list())\n",
    "rule_specific = set(rules.predicate.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicates = predicates | deverbal_nouns | status_categories | rule_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predicate_rule(\n",
    "    require_deverbal_noun: str,\n",
    "    require_reflexive: str,\n",
    "    require_status_category: str,\n",
    "    predicate: str,\n",
    "    **kwargs\n",
    "):\n",
    "    rule_id = f\"predicate={predicate},deverbal={require_deverbal_noun},reflexive={require_reflexive},status_category={require_status_category}\"\n",
    "    return rule_id, y.rule(\n",
    "        y.and_(\n",
    "            req_predicate(predicate),\n",
    "            req_deverbal(require_deverbal_noun),\n",
    "            req_reflexive(require_reflexive)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_argument_role(argument_type: str, case: str, preposition: str, **kwargs):\n",
    "    rule_id = f\"argument_type={argument_type},case={case},preposition={preposition}\"\n",
    "    arg = y.and_(\n",
    "        req_argument(),\n",
    "        req_animacy(argument_type),\n",
    "        req_case(case)\n",
    "    )\n",
    "    internal = y.and_(\n",
    "        yp.gram(\"ADJF\"), \n",
    "        y.or_(\n",
    "            yp.normalized(\"этот\"),\n",
    "            yp.normalized(\"тот\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    rule = y.or_(\n",
    "        y.rule(req_preposition(preposition), arg),\n",
    "        y.rule(req_preposition(preposition), internal, arg)\n",
    "    )\n",
    "    return rule_id, rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_deverbal(require_deverbal_noun: str = '?'):\n",
    "    if require_deverbal_noun == '1': ## strictly deverbal noun\n",
    "        return y.and_(\n",
    "            yp.gram(\"NOUN\"),\n",
    "            yp.in_caseless(deverbal_nouns)\n",
    "        )\n",
    "    elif require_deverbal_noun == '0': ## strictly regular verb\n",
    "        return y.or_(\n",
    "            yp.gram(\"VERB\"),\n",
    "            yp.gram(\"INFN\")\n",
    "        )\n",
    "    elif require_deverbal_noun == '?': ## anything\n",
    "        return y.or_(\n",
    "            y.and_(\n",
    "                yp.gram(\"NOUN\"),\n",
    "                yp.in_caseless(deverbal_nouns)\n",
    "            ),\n",
    "            yp.gram(\"VERB\"),\n",
    "            yp.gram(\"INFN\")\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect deverbal status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_reflexive(reflexive_status: str = '?'):\n",
    "    \n",
    "    def is_reflexive_verb(verb: str):\n",
    "        return verb.endswith(\"ся\") or verb.endswith(\"сь\")\n",
    "    \n",
    "    if reflexive_status == \"1\":\n",
    "        return yp.custom(is_reflexive_verb)\n",
    "    if reflexive_status == \"0\":\n",
    "        return y.not_(yp.custom(is_reflexive_verb))\n",
    "    elif reflexive_status == \"?\":\n",
    "        return yp.true()\n",
    "    else:\n",
    "        raise ValueError (\"Incorrect reflexive status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_animacy(animacy: str = 'любой'):\n",
    "    if animacy == 'любой':\n",
    "        return yp.true()\n",
    "    elif animacy == 'одуш.':\n",
    "        return y.or_(\n",
    "            y.not_(yp.gram('inan')),\n",
    "            yp.gram(\"anim\"),\n",
    "            yp.gram(\"NPRO\"),\n",
    "            yp.gram(\"ADJF\")\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect Animacy Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_argument():\n",
    "    return y.and_(\n",
    "        y.not_(\n",
    "            y.or_( ## prohibits arguments from being any of following parts-of-speech\n",
    "                yp.gram('PREP'),\n",
    "                yp.gram(\"CONJ\"),\n",
    "                yp.gram('PRCL'),\n",
    "                yp.gram(\"INTJ\"),\n",
    "                yp.gram(\"ADJF\")\n",
    "            )\n",
    "        ),\n",
    "        y.or_(\n",
    "            yp.gram(\"NOUN\"),\n",
    "            yp.gram(\"NPRO\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_predicate(word: str = \"?\"):\n",
    "    predicate = y.or_(\n",
    "        yp.gram(\"VERB\"),\n",
    "        yp.gram(\"INFN\"),\n",
    "        yp.gram(\"NOUN\")\n",
    "    )\n",
    "    if word != '?':\n",
    "        predicate = y.and_(\n",
    "            yp.normalized(word),\n",
    "            predicate\n",
    "        )\n",
    "        \n",
    "    return predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_case(case: str = 'в'):\n",
    "    if case == 'в':\n",
    "        pred = yp.gram(\"accs\")\n",
    "    elif case == 'т':\n",
    "        pred = yp.gram(\"ablt\")\n",
    "    elif case == 'д':\n",
    "        pred = yp.gram('datv')\n",
    "    elif case == 'р':\n",
    "        pred = yp.gram(\"gent\")\n",
    "    elif case == 'и':\n",
    "        pred = yp.gram(\"nomn\")\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect Case\")\n",
    "    \n",
    "    return y.or_(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_preposition(preposition: str = None):\n",
    "    if preposition == 'None':\n",
    "        return y.empty()\n",
    "    else:\n",
    "        return y.and_(\n",
    "            yp.gram(\"PREP\"),\n",
    "            yp.eq(preposition)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_parser_pass(parser, text):\n",
    "    matches = []\n",
    "    for match in parser.findall(text):\n",
    "        matches.append({\n",
    "            'text': \" \".join([x.value for x in match.tokens]),\n",
    "            'span': tuple(match.span)\n",
    "        })\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_parser_pass(parser, text):\n",
    "    match = parser.match(text)\n",
    "    matches.append({\n",
    "        'text': \" \".join([x.value for x in match.tokens]),\n",
    "        'span': tuple(match.span)\n",
    "    })\n",
    "\n",
    "    return [match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rules(**kwargs):\n",
    "    predicate_rule_id, predicate_rule = create_predicate_rule(**kwargs)\n",
    "    argument_rule_id, argument_rule = create_argument_role(**kwargs)\n",
    "    return {\n",
    "        'predicate_id': predicate_rule_id,\n",
    "        'argument_id': argument_rule_id,\n",
    "        'predicate_parser': y.Parser(predicate_rule, yt.MorphTokenizer(morph=CachedMostProbMorphAnalyzer())),\n",
    "        'argument_parser': y.Parser(argument_rule, yt.MorphTokenizer(morph=CachedMostProbMorphAnalyzer()))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleset = set(rules.role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d60890d9d54f38a2fd72f18bd22387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='инструмент'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abed1c130e3b47969803fb7ab5b4f2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='каузатив'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06389a5724d84422b28c122c41db2634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='каузатор'), FloatProgress(value=0.0, max=31.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7454382c294ccda2b5f1b524b1868a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='объект'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4a52854a2f4b9fa6ec48a57e7abc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='экспериенцер'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for role in roleset:\n",
    "    ruleset[role] = []\n",
    "    \n",
    "    for rule_dict in tqdm(rules[rules.role == role].to_dict(orient='records'), desc=role):\n",
    "        ruleset[role].append(create_rules(**rule_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument_rules = {}\n",
    "for role in ruleset.keys():\n",
    "    for rule in ruleset[role]:\n",
    "        argument_rules[f\"{rule['argument_id']}+{role}\"] = {\n",
    "            'role':role,\n",
    "            'argument_parser': rule['argument_parser']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_orient = {}\n",
    "for role in ruleset.keys():\n",
    "    for rule in ruleset[role]:\n",
    "        rule_id = rule['predicate_id']\n",
    "        if rule_id not in predicate_orient:\n",
    "            predicate_orient[rule_id] = {}\n",
    "            predicate_orient[rule_id]['predicate_parser'] = rule['predicate_parser']\n",
    "            predicate_orient[rule_id]['arguments'] = []\n",
    "            \n",
    "        predicate_orient[rule_id]['arguments'].append({\n",
    "            'role': role,\n",
    "            'argument_id': rule['argument_id'],\n",
    "            'argument_parser': rule['argument_parser']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_orient_rules = {}\n",
    "for predicate_id in predicate_orient.keys():\n",
    "    argument_tuples = set([\n",
    "        f\"{x['argument_id']}+{x['role']}\" for x in predicate_orient[predicate_id]['arguments']\n",
    "    ])\n",
    "    predicate_orient_rules[predicate_id] = {\n",
    "        'predicate_parser': predicate_orient[predicate_id]['predicate_parser'],\n",
    "        'arguments': [argument_rules[key] for key in argument_tuples]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yargy.pipelines as pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pipeline = y.Parser(\n",
    "    pipelines.morph_pipeline(list(all_predicates)),\n",
    "    tokenizer=yt.MorphTokenizer(\n",
    "        morph=CachedMostProbMorphAnalyzer()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parseable(text, parser):\n",
    "    return len(list(parser.findall(text))) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_parseable(\"Вашингтон возмущается бездействием Москвы\", filter_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_parseable(\"коррелируют\", filter_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgumentExtractor:\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def extract(self, sentence: str) -> List[Dict[str, Any]]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline, ProcessingError\n",
    "from predpatt import PredPatt, load_conllu\n",
    "from predpatt import PredPattOpts\n",
    "from predpatt.util.ud import dep_v1, dep_v2\n",
    "\n",
    "class PredPattArgumentExtractor(ArgumentExtractor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_to_udpipe: str,\n",
    "        resolve_relcl: bool = True,\n",
    "        resolve_appos: bool = True,\n",
    "        resolve_amod: bool = True,\n",
    "        resolve_conj: bool = True,\n",
    "        resolve_poss: bool = True,\n",
    "        ud = dep_v2.VERSION\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = Model.load(path_to_udpipe)\n",
    "        self.pipeline = Pipeline(self.model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "        self._error = ProcessingError()\n",
    "        self._opts = PredPattOpts(\n",
    "            resolve_relcl=resolve_relcl,\n",
    "            resolve_appos=resolve_appos,\n",
    "            resolve_amod=resolve_amod,\n",
    "            resolve_conj=resolve_conj,\n",
    "            resolve_poss=resolve_poss,\n",
    "            ud=ud\n",
    "        )\n",
    "        \n",
    "    def extract(self, sentence: str) -> List[Dict[str, Any]]:\n",
    "        processed = self.pipeline.process(sentence, self._error)\n",
    "        if self._error.occurred():\n",
    "            print(f\"=== Error occurred: {self._error.message}\")\n",
    "            self._error = ProcessingError()\n",
    "            return None\n",
    "        else:\n",
    "            conll_example = [ud_parse for sent_id, ud_parse in load_conllu(processed)][0]\n",
    "            ppatt = PredPatt(conll_example, opts=self._opts)\n",
    "            result = []\n",
    "            for predicate in ppatt.instances:\n",
    "                structure = {\n",
    "                    'predicate': predicate.tokens,\n",
    "                    'arguments': [x.tokens for x in predicate.arguments]\n",
    "                }\n",
    "                result.append(structure)\n",
    "                \n",
    "            return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class MainPhraseExtractor:\n",
    "    \n",
    "    def __init__(self, syntax_parser, pymorphy_analyzer):\n",
    "        self.syntax = syntax_parser\n",
    "        self.morph = pymorphy_analyzer\n",
    "\n",
    "    def get_main_phrase(self, words, get_prep=False, verbose=False):\n",
    "        markup = next(self.syntax.map([words]))\n",
    "        forward = {}\n",
    "        backward = defaultdict(list)\n",
    "        token_map = {}\n",
    "        candidates = []\n",
    "        for token in markup.tokens:\n",
    "            if token.head_id not in backward:\n",
    "                backward[token.head_id] = []\n",
    "\n",
    "            token_map[token.id] = token\n",
    "            forward[token.id] = token.head_id\n",
    "            backward[token.head_id].append(token.id)\n",
    "\n",
    "            if token.id == token.head_id or token.head_id == '0':\n",
    "                candidates.append(token.id)\n",
    "             \n",
    "        if verbose:\n",
    "            print(\"forward \", forward)\n",
    "            print(\"backward \", backward)\n",
    "            print(\"candidates \", candidates)\n",
    "                \n",
    "        if len(candidates) == 0:\n",
    "            return markup.tokens\n",
    "\n",
    "        candidate = sorted(candidates, key=lambda x: len(backward[x]))[-1]\n",
    "        if get_prep:\n",
    "            prep_candidates = backward[candidate]\n",
    "            prep_candidates = list(\n",
    "                filter(lambda x: self.morph.tag(token_map[x].text)[0].POS == 'PREP', prep_candidates)\n",
    "            )\n",
    "            if len(prep_candidates) == 0:\n",
    "                return [token_map[candidate]]\n",
    "            \n",
    "            prep = sorted(prep_candidates, key=lambda x: abs(int(x) - int(candidate)))[0]\n",
    "            return (token_map[prep], token_map[candidate])\n",
    "\n",
    "        return [token_map[candidate]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RstClauseSeparator:\n",
    "    def __init__(self, udpipe=('tsa05.isa.ru', 3334), rst=('tsa05.isa.ru', 3335), cache_path=\"./rst-cache.pkl\"):\n",
    "        udpipe_host, udpipe_port = udpipe\n",
    "        rst_host, rst_port = rst\n",
    "        self.cache_path = cache_path\n",
    "        self.ppl = PipelineCommon([\n",
    "            (ProcessorRemote(udpipe_host, udpipe_port, '0'),\n",
    "             ['text'],\n",
    "             {'sentences': 'sentences',\n",
    "              'tokens': 'tokens',\n",
    "              'lemma': 'lemma',\n",
    "              'syntax_dep_tree': 'syntax_dep_tree',\n",
    "              'postag': 'ud_postag'}),\n",
    "            (ProcessorMystem(delay_init=False),\n",
    "             ['tokens', 'sentences'],\n",
    "             {'postag': 'postag'}),\n",
    "            (ConverterMystemToUd(),\n",
    "             ['postag'],\n",
    "             {'morph': 'morph',\n",
    "              'postag': 'postag'}),\n",
    "            (ProcessorRemote(rst_host, rst_port, 'default'),\n",
    "             ['text', 'tokens', 'sentences', 'postag', 'morph', 'lemma', 'syntax_dep_tree'],\n",
    "             {'rst': 'rst'})\n",
    "        ])\n",
    "        self.__cache = {}\n",
    "        self.__hasher = city_32()\n",
    "        if os.path.exists(self.cache_path):\n",
    "            self.__cache = jb.load(self.cache_path)\n",
    "        \n",
    "    def extract(self, text):\n",
    "        text_hash = self.__hasher(text)\n",
    "        if text_hash in self.__cache:\n",
    "            return self.__cache[text_hash]\n",
    "        else:\n",
    "            result = self.ppl(text)\n",
    "            clauses = [x.text for x in result['rst']]\n",
    "            self.__cache[text_hash] = clauses\n",
    "            return clauses\n",
    "        \n",
    "        \n",
    "    def flush(self):\n",
    "        jb.dump(self.__cache, self.cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoleLabeler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        argument_extractor: ArgumentExtractor,\n",
    "        main_phrase_extractor: MainPhraseExtractor,\n",
    "        filter_pipeline,\n",
    "        predicate_ruleset,\n",
    "        mode: str = 'soft'\n",
    "    ):\n",
    "        \n",
    "        self.argument_extractor = argument_extractor\n",
    "        self.main_phrase_extractor = main_phrase_extractor\n",
    "        self.filter_pipeline = filter_pipeline\n",
    "        self.ruleset = predicate_ruleset\n",
    "        if mode == 'soft':\n",
    "            self.pass_fn = soft_parser_pass\n",
    "        elif mode == 'strict':\n",
    "            self.pass_fn = strict_parser_pass\n",
    "        else:\n",
    "            raise ValueError(f\"Incorrect mode = {mode}, can be 'soft' or 'strict'\")\n",
    "            \n",
    "    def check_parse(self, text, parser):\n",
    "        return len(self.pass_fn(parser, text)) > 0\n",
    "    \n",
    "    def run(self, sentence):\n",
    "        arg_groups = self.argument_extractor.extract(sentence)\n",
    "        arg_groups = list(\n",
    "            filter(\n",
    "                lambda x: check_parseable(\n",
    "                    \" \".join([token.text for token in x['predicate']]),\n",
    "                    self.filter_pipeline\n",
    "                ),\n",
    "                arg_groups\n",
    "            )\n",
    "        )\n",
    "        result = []\n",
    "        for group in arg_groups:\n",
    "            \n",
    "            predicate_txt = \" \".join([token.text for token in group['predicate']])\n",
    "            predicate_tokens = [token.text for token in group['predicate']]\n",
    "            predicate_main = \" \".join([x.text for x in self.main_phrase_extractor.get_main_phrase(predicate_tokens)])\n",
    "            forward_map = {\" \".join([token.text for token in argument]): argument for argument in group['arguments']}\n",
    "            group_name = f\"predicate={predicate_txt},arguments=[{','.join(forward_map.keys())}]\"\n",
    "            group_result = []\n",
    "            for predicate in self.ruleset.values():\n",
    "                if self.check_parse(predicate_main, predicate['predicate_parser']):\n",
    "                    predicate_result = {\n",
    "                        'predicate': predicate_txt,\n",
    "                        'predicate_analyzed': predicate_main,\n",
    "                        'predicate_tokens': group['predicate'],\n",
    "                        'arguments': []\n",
    "                    }\n",
    "                    for argument in forward_map.keys():\n",
    "                        argument_tokens = [x.text for x in forward_map[argument]]\n",
    "                        argument_main = \" \".join([\n",
    "                            x.text for x in self.main_phrase_extractor.get_main_phrase(argument_tokens, True)\n",
    "                        ])\n",
    "                        roles = [\n",
    "                            rule['role'] for rule in predicate['arguments'] \n",
    "                            if self.check_parse(argument_main, rule['argument_parser'])\n",
    "                        ]\n",
    "                        if len(roles) > 0: \n",
    "                            predicate_result['arguments'].append({\n",
    "                                'argument': argument,\n",
    "                                'argument_analyzed': argument_main,\n",
    "                                'argument_tokens': forward_map[argument],\n",
    "                                'roles': tuple(roles)\n",
    "                            })\n",
    "                    if len(predicate_result['arguments']) > 0:\n",
    "                        predicate_result['arguments'] = tuple(predicate_result['arguments'])\n",
    "                        group_result.append(predicate_result)\n",
    "            result.append({'group': group_name, 'parses': group_result})\n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstraintEnforcer:\n",
    "    def __init__(self, constraints=None):\n",
    "        if constraints is None:\n",
    "            constraints = list()\n",
    "            \n",
    "        self.constraints = constraints\n",
    "        \n",
    "    def add(self, constraint):\n",
    "        self.constraints.append(constraint)\n",
    "        \n",
    "    def enforce(self, parse):\n",
    "        a_parse = parse.copy()\n",
    "        for constraint in self.constraints:\n",
    "            a_parse = constraint(a_parse)\n",
    "            if len(a_parse) == 0:\n",
    "                return a_parse\n",
    "            \n",
    "        return a_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "from slovnet import Syntax\n",
    "navec = Navec.load('../data/models/navec_news_v1_1B_250K_300d_100q.tar')\n",
    "syntax = Syntax.load('../data/models/slovnet_syntax_news_v1.tar')\n",
    "_ = syntax.navec(navec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_phrase_extractor = MainPhraseExtractor(syntax, MorphAnalyzer())\n",
    "extractor = PredPattArgumentExtractor(\"../data/models/russian-syntagrus-ud-2.5-191206.udpipe\")\n",
    "clause_extractor = RstClauseSeparator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = RoleLabeler(extractor, main_phrase_extractor, filter_pipeline, predicate_orient_rules, mode='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "enforcer = ConstraintEnforcer()\n",
    "def enforce_parseable_predicate(parse):\n",
    "    if check_parseable(parse['predicate_analyzed'], filter_pipeline):\n",
    "        return parse\n",
    "    else:\n",
    "        return {}\n",
    "    \n",
    "def reduce_duplicate_roles(parse):\n",
    "    new_args = []\n",
    "    for arg in parse['arguments']:\n",
    "        arg['roles'] = tuple(set(arg['roles']))\n",
    "        new_args.append(arg)\n",
    "    parse['arguments'] = new_args\n",
    "    return parse\n",
    "    \n",
    "def resolve_multiple_expirirencers(parse):\n",
    "    if len(parse['arguments']) >= 2:\n",
    "        parse_roles = set(arg['roles'] for arg in parse['arguments'])\n",
    "        if ('экспериенцер',) in parse_roles:\n",
    "            new_args = []\n",
    "            for arg in parse['arguments']:\n",
    "                if len(arg['roles']) >= 2:\n",
    "                    new_roles = list(arg['roles'])\n",
    "                    new_roles.remove('экспериенцер')\n",
    "                    arg['roles'] = tuple(new_roles)\n",
    "                new_args.append(arg)\n",
    "            parse['arguments'] = new_args\n",
    "    return parse\n",
    "\n",
    "def resolve_single_expiriencer(parse):\n",
    "    parse_roles = [arg['roles'] for arg in parse['arguments'] if len(arg['roles']) >= 2]\n",
    "    if len(parse_roles) > 0:\n",
    "        n_exp = 0\n",
    "        for role in parse_roles:\n",
    "            if 'экспериенцер' in role:\n",
    "                n_exp += 1\n",
    "                \n",
    "        if n_exp == 1:\n",
    "            new_args = []\n",
    "            for arg in parse['arguments']:\n",
    "                if len(arg['roles']) >= 2 and 'экспериенцер' in arg['roles']:\n",
    "                    arg['roles'] = ('экспериенцер',)\n",
    "                new_args.append(arg)\n",
    "            parse['arguments'] = new_args\n",
    "    return parse\n",
    "        \n",
    "enforcer.add(enforce_parseable_predicate)\n",
    "enforcer.add(reduce_duplicate_roles) # ('каузатор', 'экспериенцер', 'экспериенцер') ->  ('каузатор', 'экспериенцер')\n",
    "enforcer.add(resolve_multiple_expirirencers)\n",
    "enforcer.add(resolve_single_expiriencer) # ('каузатор', 'экспериенцер') -> ('экспериенцер')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(text, parse, main_text):\n",
    "    tokens = {i: x.text for i, x in enumerate(razdel.tokenize(text))}\n",
    "    arguments = []\n",
    "    sample_role = None\n",
    "    for i, arg in enumerate(parse['arguments']):\n",
    "        arguments.append(arg['argument_analyzed'])\n",
    "        if i == 0 or sample_role is None:\n",
    "            if len(arg['roles']) > 0:\n",
    "                sample_role = arg['roles'][0]\n",
    "        new_tokens = [x.text for x in arg['argument_tokens']]\n",
    "        new_tokens[0] = f\"[{new_tokens[0]}\"\n",
    "        new_tokens[-1] = f\"{new_tokens[-1]}#{'/'.join(set(arg['roles']))}]\"\n",
    "        new_tokens = {arg['argument_tokens'][i].position: new_tokens[i] for i in range(len(new_tokens))}\n",
    "        tokens = {**tokens, **new_tokens}\n",
    "        \n",
    "    new_tokens = [x.text for x in parse['predicate_tokens']]\n",
    "    target_token = parse['predicate_analyzed'].split()[-1] # taking last word of analyzed part of predicate phrase\n",
    "    target_idx = new_tokens.index(target_token)\n",
    "    new_tokens[0] = f\"[{new_tokens[0]}\"\n",
    "    new_tokens[target_idx] = f\"{new_tokens[target_idx]}@Предикат\"\n",
    "    new_tokens[-1] = f\"{new_tokens[-1]}]\"\n",
    "    new_tokens = {parse['predicate_tokens'][i].position: new_tokens[i] for i in range(len(new_tokens))}\n",
    "    tokens = {**tokens, **new_tokens}\n",
    "    tokens = sorted(tokens.items(), key=lambda x: x[0])\n",
    "    return {\n",
    "        'main_text': main_text,\n",
    "        'formatted_clause_text': \" \".join([x[1] for x in tokens]),\n",
    "        'orig_clause_text': text,\n",
    "        'predicate': parse['predicate_analyzed'],\n",
    "        'arguments': arguments,\n",
    "        'role': sample_role\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [list(x.iterdir()) for x in Path(\"../data/txts/\").iterdir()]\n",
    "cats = list(chain.from_iterable(cats))\n",
    "cats = list(chain.from_iterable([x.iterdir() for x in cats]))\n",
    "files = list(chain.from_iterable([x.iterdir() for x in cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9103af0852c48cb9eadad4aea94110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9429.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for file in tqdm(files):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        sentences += list(razdel.sentenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [x.text for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576f0f98766c48d1a8ff44183c96055b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "parse_res = []\n",
    "groups_res = []\n",
    "parses = []\n",
    "for text in tqdm(sentences[:100000]):\n",
    "    clauses = clause_extractor.extract(text)\n",
    "    for clause in clauses:\n",
    "        groups = labeler.run(clause)\n",
    "        groups_res += groups\n",
    "        for group in groups:\n",
    "            for parse in group['parses']:\n",
    "                parse = enforcer.enforce(parse)\n",
    "                if len(parse) != 0:\n",
    "                    try:\n",
    "                        parses.append(parse)\n",
    "                        parse_res.append(visualize(clause, parse, text))\n",
    "                    except Exception:\n",
    "                        print(f\"== Error with {text}\")\n",
    "                        print(traceback.format_exc())\n",
    "clause_extractor.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_extractor.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parsed-100k.pkl']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jb.dump(parses, \"parsed-100k.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag2str(tag):\n",
    "    return \",\".join(sorted(tag.grammemes_cyr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def get_most_prob_tags(word, morph):\n",
    "    variants = morph.parse(word)\n",
    "    max_score = max(x.score for x in variants)\n",
    "    variants = filter(\n",
    "        lambda x: x.score == max_score,\n",
    "        variants\n",
    "    )\n",
    "    variants = list(map(lambda x: x.tag, variants))\n",
    "    return variants\n",
    "\n",
    "def get_morph_string(parse):\n",
    "    predicate = [x.text for x in razdel.tokenize(parse['predicate'])]\n",
    "    arguments = parse['arguments']\n",
    "    arguments = [[x.text for x in razdel.tokenize(arg)] for arg in arguments]\n",
    "    predicate_tags = [get_most_prob_tags(x, morph) for x in predicate]\n",
    "    arguments_tags = [[get_most_prob_tags(x, morph) for x in arg] for arg in arguments]\n",
    "    \n",
    "    arguments_str = \"\"\n",
    "    for i in range(len(arguments)):\n",
    "        words = arguments[i]\n",
    "        for j in range(len(words)):\n",
    "            word = words[j]\n",
    "            tags = arguments_tags[i][j]\n",
    "            tags = list(map(tag2str, tags))\n",
    "            arg_str = f\"{word}#[{'/'.join(tags)}]\"\n",
    "            arguments_str += arg_str + \";\"\n",
    "            \n",
    "    predicate_str = \"\"\n",
    "    for i in range(len(predicate)):\n",
    "        word = predicate[i]\n",
    "        tags = predicate_tags[i]\n",
    "        tags = list(map(tag2str, tags))\n",
    "        predicate_str += f\"{word}#[{'/'.join(tags)}]\"\n",
    "        \n",
    "    result_str = f\"Предикат={predicate_str}||Аргументы={arguments_str}\"\n",
    "        \n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Предикат=беситесь#[2л,ГЛ,изъяв,мн,наст,неперех,несов/ГЛ,выкл,мн,неперех,несов,повел]||Аргументы=вы#[2л,МС,им,мн];'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_morph_string(parse_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccee62ad23364282adc332476bbc9274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=638.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for obj in tqdm(parse_res):\n",
    "    obj['morph'] = get_morph_string(obj)\n",
    "    obj['arguments'] = \";\".join(obj['arguments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main_text': 'Включить рециркуляцию автокондиционера летом - это, дружище, вы от злобы беситесь.',\n",
       " 'formatted_clause_text': 'Включить рециркуляцию автокондиционера летом - [это , дружище , [вы#экспериенцер] от злобы беситесь@Предикат] .',\n",
       " 'orig_clause_text': 'Включить рециркуляцию автокондиционера летом - это, дружище, вы от злобы беситесь.',\n",
       " 'predicate': 'беситесь',\n",
       " 'arguments': 'вы',\n",
       " 'role': 'экспериенцер',\n",
       " 'morph': 'Предикат=беситесь#[2л,ГЛ,изъяв,мн,наст,неперех,несов/ГЛ,выкл,мн,неперех,несов,повел]||Аргументы=вы#[2л,МС,им,мн];'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = pd.DataFrame(parse_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse['_pr'] = parse['predicate'] + \"+\" + parse['role']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse.to_csv(\"parsed_roles-100k.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = parse['_pr'].value_counts().to_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = parse.sample(frac=1.0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9480b5f0dc4951bfe9161cb3a3fc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=380.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "refined = []\n",
    "for pair in tqdm(pairs):\n",
    "    refined += parse[parse['_pr'] == pair].head(10).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parse = pd.DataFrame(refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parse = new_parse.loc[:, ['role','predicate','arguments','formatted_clause_text', 'morph', 'main_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parse = new_parse.drop_duplicates(subset='formatted_clause_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parse.to_csv(\"./parsed_roles-100k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "_interest = new_parse.formatted_clause_text.str.contains(\"@Предикат \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    494\n",
       "True      93\n",
       "Name: formatted_clause_text, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_interest.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
